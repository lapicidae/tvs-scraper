#!/usr/bin/env python3
"""
A lean web scraper for TVSpielfilm.de to extract TV program data.

This script uses the `requests` library for HTTP requests and `lxml.html`
with `cssselect` for efficient HTML parsing. It supports custom
file-based caching for HTTP responses and processed JSON data, parallel fetching
using a thread pool, and robust retry mechanisms for network and
application-level errors. It generates EPG data in XMLTV or JSON format.
"""
import argparse
import concurrent.futures
import functools
import json
import logging
import logging.handlers
import lxml.cssselect
import lxml.etree
import lxml.html
import os
import portalocker
import random
import re
import requests
import shutil
import sys
import tempfile
import threading
import time
from argparse import RawTextHelpFormatter
from datetime import datetime, timedelta, timezone
from requests.adapters import HTTPAdapter
from typing import List, Dict, Optional, Any, Tuple
from urllib.parse import urlparse, parse_qs
from urllib3.util.retry import Retry

if sys.version_info < (3, 9):
    print("Error: This script requires Python 3.9 or higher for the 'zoneinfo' module. Please update your Python version.")
    sys.exit(1)
from zoneinfo import ZoneInfo

logger = logging.getLogger(__name__)

DEFAULT_DAYS: int = 0
DEFAULT_IMAGE_SIZE: str = "600"
DEFAULT_IMAGE_CHECK: bool = False
MAX_DAYS_TO_SCRAPE: int = 13

DEFAULT_CACHE_SUBDIRECTORY: str = "tvs-cache"
DEFAULT_CACHE_DISABLE: bool = False
DEFAULT_CACHE_TTL_SECONDS: int = 24 * 60 * 60 # 24 hours
CACHE_PROCESSED_DATA_SUBDIRECTORY: str = "processed-data"
CACHE_CHANNEL_LIST_FILE: str = "channels.json"

DEFAULT_MAX_WORKERS: int = 10
DEFAULT_MAX_RETRIES: int = 5
RETRY_STATUS_FORCELIST: List[int] = [429, 500, 502, 503, 504]
RETRY_BACKOFF_FACTOR: float = 1.0

DEFAULT_OUTPUT_FORMAT: str = 'xmltv'
DEFAULT_SYSLOG_TAG: str = 'tvs-scraper'
DEFAULT_MIN_REQUEST_DELAY: float = 0.05
DEFAULT_MAX_CONCURRENT_REQUESTS: int = 15
DEFAULT_MAX_SCHEDULE_RETRIES: int = 3
DEFAULT_CACHE_VALIDATION_TOLERANCE: int = 5
DEFAULT_HTTP_TIMEOUT: int = 10

USER_AGENTS: List[str] = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:138.0) Gecko/20100101 Firefox/138.0',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36 Edg/136.0.0.0',
    'Mozilla/5.0 (X11; Linux x86_64; rv:138.0) Gecko/20100101 Firefox/138.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/136.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36',
]

BASE_URL: str = "https://m.tvspielfilm.de"
CHANNEL_LIST_URL: str = f"{BASE_URL}/sender/"

re_cast_entry_match = re.compile(r'(.+?)\s*\((.+?)\)')

_INVALID_XML_CHARS_PATTERN = re.compile(
    '[\x00-\x08\x0B\x0C\x0E-\x1F\x7F-\x9F\uFDD0-\uFDEF\uFFFE\uFFFF]'
)

def _sanitize_xml_string(text: Optional[str]) -> Optional[str]:
    """
    Removes characters from a string that are invalid in XML 1.0 documents.

    :param text: The input string.
    :type text: Optional[str]
    :returns: The sanitized string, or None if input was None.
    :rtype: Optional[str]
    """
    if text is None:
        return None
    text = str(text)
    return _INVALID_XML_CHARS_PATTERN.sub('', text)


def _convert_unix_to_xmltv_time(unix_timestamp_str: Optional[str], target_timezone_str: str = 'UTC') -> Optional[str]:
    """
    Converts a Unix timestamp string (in seconds) to XMLTV time format (YYYYMMDDhhmmss +ZZZZ).

    :param unix_timestamp_str: Unix timestamp as a string.
    :type unix_timestamp_str: Optional[str]
    :param target_timezone_str: The target timezone string (e.g., 'Europe/Berlin').
    :type target_timezone_str: str
    :returns: XMLTV formatted time string, or None if conversion fails.
    :rtype: Optional[str]
    """
    if not unix_timestamp_str:
        return None
    try:
        unix_timestamp: int = int(unix_timestamp_str)

        if unix_timestamp < 946684800:  # Sat Jan 01 2000 00:00:00 GMT+0000
            logger.warning(f"Unrealistically old Unix timestamp '{unix_timestamp_str}' for XMLTV conversion received. Skipping.")
            return None

        dt_object_utc: datetime = datetime.fromtimestamp(unix_timestamp, tz=timezone.utc)

        try:
            target_tz = ZoneInfo(target_timezone_str)
        except Exception as e:
            logger.error(f"Unknown timezone '{target_timezone_str}'. Falling back to UTC. Error: {e}")
            target_tz = timezone.utc

        dt_object_local: datetime = dt_object_utc.astimezone(target_tz)

        offset_seconds = dt_object_local.utcoffset().total_seconds()
        offset_hours = int(offset_seconds // 3600)
        offset_minutes = int((abs(offset_seconds) % 3600) // 60)
        offset_sign = '+' if offset_hours >= 0 else '-'
        offset_str = f"{offset_sign}{abs(offset_hours):02d}{abs(offset_minutes):02d}"

        return dt_object_local.strftime(f'%Y%m%d%H%M%S {offset_str}')
    except (ValueError, TypeError) as e:
        logger.warning(f"Could not convert Unix timestamp '{unix_timestamp_str}' to XMLTV time: {e}")
        return None


def _process_rating_string(original_rating_string: Optional[str]) -> Optional[str]:
    """
    Cleans and formats the rating string.

    :param original_rating_string: The raw rating string.
    :type original_rating_string: Optional[str]
    :returns: The cleaned and formatted rating string, or None if input was None.
    :rtype: Optional[str]
    """
    if not original_rating_string:
        return None
    parts: List[str] = [p.strip() for p in original_rating_string.split('/')]
    filtered_parts: List[str] = [part for part in parts if '*' in part]
    return "/ " + " / ".join(filtered_parts) if filtered_parts else None


class TvsHtmlParser:
    """
    Encapsulates all HTML parsing logic for TVSpielfilm.de.
    """
    def __init__(self):
        """
        Initializes the TvsHtmlParser with all necessary CSS selectors
        and regular expressions for parsing.
        """
        self.re_source_id_html: re.Pattern = re.compile(r',([a-zA-Z0-9_-]+)\.html$')
        self.re_image_url_match: re.Pattern = re.compile(r"url\(['\"]?(.*?)['\"]?\)")
        self.re_genre_pipe_match: re.Pattern = re.compile(r'\|\s*(.*)')
        self.re_duration_minutes_match: re.Pattern = re.compile(r'(\d+)\s*Min\.', re.IGNORECASE)
        self.re_fsk_match: re.Pattern = re.compile(r'(\d+)')
        self.re_rating_class_match: re.Pattern = re.compile(r'rating-(\d+)')
        self.re_adsc_sparte_match: re.Pattern = re.compile(r'"adsc_sparte":"([^"]+)"')
        self.re_season_episode_match: re.Pattern = re.compile(r'Staffel\s*(\d+),\s*Folge\s*(\d+)(?:/(\d+))?')

        self.selector_sender_links: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.component.channels.all-channels.abc-scroll ul li a')
        self.selector_li_elements: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('li.tv-tip.time-listing.js-tv-show.channels')
        self.selector_title_tag: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('strong.tv-tip-heading span.title')
        self.selector_detail_link_tag: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.image-text-holder a.flex-row.js-track-link')
        self.selector_image_div: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.program-image div.default-image')

        self.base_detail_list_container: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.detail-tab__container-content[data-detail-tab-index="1"] div.list-container div.definition-list')
        self.selector_dl: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('dl')
        self.selector_dt: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('dt')
        self.selector_dd: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('dd')
        self.selector_headline_p: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('p.headline')

        self.selector_description_section: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('section.broadcast-detail__description')
        self.selector_short_desc_tag: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('p.headline')
        self.selector_long_desc_paragraphs: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('p:not(.headline)')
        self.selector_genre_underline_tag: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.stage-underline.gray span.text-row')
        self.selector_genre_tag_general: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.genre-info span.genre-text')
        self.selector_genre_time_tag_schedule: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.genre span.genre-time')
        self.selector_content_rating_section: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('section.content-rating')
        self.selector_num_rating_div: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.content-rating__rating-genre__thumb')
        self.selector_txt_rating_tag: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('blockquote.content-rating__rating-genre__conclusion-quote p')
        self.selector_rating_list_items: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('ul.content-rating__rating-genre__list li.content-rating__rating-genre__list-item')
        self.selector_rating_label: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('span.content-rating__rating-genre__list-item__label')
        self.selector_rating_span: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('span.content-rating__rating-genre__list-item__rating')
        self.selector_imdb_rating_tag: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.content-rating__imdb-rating__rating-value')
        self.selector_tipp_tag: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.content-rating__top-rated')

        self.selector_subheadline: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('h2.broadcast-info')
        self.selector_season_episode: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('section.serial-info span')

        self.selector_broadcast_guests_container: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('div.broadcast-info-guests')
        self.selector_guests_list: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('ul.broadcast-guests')
        self.selector_guests_title: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('li.title')
        self.selector_guest_names: lxml.cssselect.CSSSelector = lxml.cssselect.CSSSelector('li:not(.title)')

    def _parse_html_root(self, html_content: str, context_info: str = "HTML content") -> Optional[lxml.html.HtmlElement]:
        """
        Parses HTML content into an lxml HtmlElement, with robust error handling.

        :param html_content: The HTML content string.
        :type html_content: str
        :param context_info: A string describing the context of the HTML (e.g., "channel list page").
        :type context_info: str
        :returns: The parsed lxml HtmlElement, or None if parsing fails.
        :rtype: Optional[lxml.html.HtmlElement]
        """
        try:
            root: lxml.html.HtmlElement = lxml.html.fromstring(html_content)
            return root
        except lxml.etree.XMLSyntaxError as e:
            logger.error(f"XMLSyntaxError when parsing {context_info}: {e}. Returning None.")
            return None
        except Exception as e:
            logger.error(f"Unexpected error when parsing {context_info}: {e}. Returning None.")
            return None

    def _normalize_string_for_comparison(self, text: Optional[str]) -> str:
        """
        Normalizes a string for case-insensitive and special-character-agnostic comparison.
        Removes non-alphanumeric characters and converts to lowercase.

        :param text: The input string.
        :type text: Optional[str]
        :returns: The normalized string.
        :rtype: str
        """
        if text is None:
            return ""
        return re.sub(r'[^a-z0-9äöüß]', '', text.lower())

    def _extract_dl_data(self, dl_element: lxml.html.HtmlElement, data_mapping: Dict[str, str], detail_data: Dict[str, Any]) -> None:
        """
        Extracts data from a <dl> (definition list) HTML element based on a mapping.

        :param dl_element: The lxml HtmlElement representing the <dl> tag.
        :type dl_element: lxml.html.HtmlElement
        :param data_mapping: A dictionary mapping <dt> text content to target keys in `detail_data`.
        :type data_mapping: Dict[str, str]
        :param detail_data: The dictionary to update with extracted data.
        :type detail_data: Dict[str, Any]
        """
        dt_elements: List[lxml.html.HtmlElement] = self.selector_dt(dl_element)
        dd_elements: List[lxml.html.HtmlElement] = self.selector_dd(dl_element)

        for dt, dd in zip(dt_elements, dd_elements):
            dt_text: str = dt.text_content().strip()
            dd_text: str = dd.text_content().strip()

            if dd_text and dt_text in data_mapping:
                target_key = data_mapping[dt_text]
                sanitized_dd_text: Optional[str] = _sanitize_xml_string(dd_text)

                if target_key == 'year':
                    try:
                        detail_data[target_key] = int(dd_text)
                    except ValueError:
                        logger.debug(f"Could not parse year from '{dd_text}'.")
                elif target_key == 'duration':
                    duration_minutes_match: Optional[re.Match[str]] = self.re_duration_minutes_match.search(dd_text)
                    if duration_minutes_match:
                        detail_data[target_key] = int(duration_minutes_match.group(1)) * 60
                    else:
                        logger.debug(f"Could not parse duration from '{dd_text}'.")
                elif target_key == 'parentalrating':
                    fsk_match: Optional[re.Match[str]] = self.re_fsk_match.search(dd_text)
                    if fsk_match:
                        try:
                            detail_data[target_key] = int(fsk_match.group(1))
                        except ValueError:
                            logger.debug(f"Could not parse FSK rating from '{fsk_match.group(1)}'.")
                elif target_key == 'screenplay':
                    if sanitized_dd_text:
                        if detail_data[target_key]:
                            detail_data[target_key] = f"{detail_data[target_key]}, {sanitized_dd_text}"
                        else:
                            detail_data[target_key] = sanitized_dd_text
                else:
                    detail_data[target_key] = sanitized_dd_text

    def parse_channel_list(self, html_content: str) -> List[Dict[str, str]]:
        """
        Parses the list of channels from the HTML content of the main sender page.

        :param html_content: The HTML content of the sender page.
        :type html_content: str
        :returns: A list of dictionaries, each containing 'name', 'url', and 'source_id' for a channel.
        :rtype: List[Dict[str, str]]
        """
        root: Optional[lxml.html.HtmlElement] = self._parse_html_root(html_content, "channel list HTML")
        if root is None:
            return []

        sender_links: List[lxml.html.HtmlElement] = self.selector_sender_links(root)
        logger.debug(f"Found {len(sender_links)} potential channel links.")

        channel_data: List[Dict[str, str]] = []
        for link in sender_links:
            href: Optional[str] = link.get('href')
            name: Optional[str] = None
            if link.text_content():
                name = _sanitize_xml_string(link.text_content().strip())
            else:
                logger.debug(f"Channel link found with no text content for href: {href}. Skipping.")
                continue

            source_id: Optional[str] = None
            if href:
                source_id_match: Optional[re.Match[str]] = self.re_source_id_html.search(href)
                if source_id_match:
                    source_id = source_id_match.group(1).lower()
            if not source_id:
                tracking_data_str: Optional[str] = link.get('data-tracking-point')
                if tracking_data_str:
                    try:
                        tracking_data: Dict[str, Any] = json.loads(tracking_data_str)
                        if isinstance(tracking_data, dict) and 'channel' in tracking_data and isinstance(tracking_data['channel'], (str, int)):
                            source_id = str(tracking_data['channel']).lower()
                        else:
                            logger.debug(f"Tracking data for {name} has unexpected structure: {tracking_data_str}")
                    except json.JSONDecodeError as e:
                        logger.debug(f"Could not parse data-tracking-point for {name}: {e}")
                    except Exception as e:
                        logger.debug(f"Unexpected error processing data-tracking-point for {name}: {e}")

            if source_id and name:
                logger.debug(f"Processing channel: {name} (ID: {source_id}, Href: {href})")
                channel_data.append({
                    'name': name,
                    'url': f"{BASE_URL}{href}" if href and href.startswith('/') else (href if href else ''),
                    'source_id': source_id
                })
            else:
                logger.debug(f"Could not extract source_id for channel: {name} (Href: {href}). Skipping.")
        return channel_data

    def parse_daily_schedule_items(self, html_content: str, channel_info: Dict[str, str], current_date: datetime.date) -> List[Dict[str, Any]]:
        """
        Parses the schedule for a single channel and a specific date from the HTML content.
        This method extracts basic program information and detail links/image paths.

        :param html_content: The HTML content of the schedule page.
        :type html_content: str
        :param channel_info: Dictionary containing channel information ('name', 'url', 'source_id').
        :type channel_info: Dict[str, str]
        :param current_date: The date for which the schedule is being parsed.
        :type current_date: datetime.date
        :returns: A list of dictionaries, each representing a program item,
                  with basic information and full image URLs.
        :rtype: List[Dict[str, Any]]
        """
        channel_name: str = channel_info['name']
        channel_source_id: str = channel_info['source_id']

        root: Optional[lxml.html.HtmlElement] = self._parse_html_root(html_content, f"daily schedule HTML for channel {channel_name} on {current_date.strftime('%Y-%m-%d')}")
        if root is None:
            return []

        extracted_items: List[Dict[str, Any]] = []

        for li_element in self.selector_li_elements(root):
            item_data: Dict[str, Any] = {}

            item_data['type'] = 'program'
            item_data['channelname'] = channel_name
            item_data['sourceid'] = channel_source_id
            item_data['date'] = current_date.strftime('%Y-%m-%d')

            data_start_time = li_element.get('data-start-time')
            if data_start_time:
                item_data['starttime'] = data_start_time
            else:
                logger.debug(f"Missing 'data-start-time' for an item in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}. Skipping item.")
                continue

            data_end_time = li_element.get('data-end-time')
            if data_end_time:
                item_data['endtime'] = data_end_time
            else:
                logger.debug(f"Missing 'data-end-time' for item starting at {item_data['starttime']} in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}.")
                item_data['endtime'] = None

            title_tag: List[lxml.html.HtmlElement] = self.selector_title_tag(li_element)
            if title_tag and title_tag[0].text_content():
                item_data['title'] = _sanitize_xml_string(title_tag[0].text_content().strip())
            else:
                logger.error(f"Missing title for item starting at {item_data['starttime']} in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}. Skipping entire item due to missing title.")
                continue

            detail_link_tag: List[lxml.html.HtmlElement] = self.selector_detail_link_tag(li_element)
            if detail_link_tag and detail_link_tag[0].get('href'):
                detail_href: str = detail_link_tag[0].get('href')
                if not detail_href.startswith('http'):
                    item_data['link'] = f"{BASE_URL}{detail_href}"
                else:
                    item_data['link'] = detail_href
            else:
                logger.debug(f"Missing detail link tag or href for item '{item_data['title']}' in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}.")
                item_data['link'] = None

            data_id = li_element.get('data-id')
            if data_id:
                item_data['eventid'] = data_id
            else:
                logger.debug(f"Missing 'data-id' for item '{item_data['title']}' in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}.")
                item_data['eventid'] = None

            image_div: List[lxml.html.HtmlElement] = self.selector_image_div(li_element)
            if image_div and image_div[0].get('style'):
                style_attr: str = image_div[0].get('style')
                image_url_match: Optional[re.Match[str]] = self.re_image_url_match.search(style_attr)
                if image_url_match:
                    item_data['image_url'] = image_url_match.group(1)
                else:
                    logger.debug(f"No image URL found in style attribute for item '{item_data['title']}' in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}.")
                    item_data['image_url'] = None
            else:
                logger.debug(f"Missing image div or style attribute for item '{item_data['title']}' in {channel_name}'s schedule on {current_date.strftime('%Y-%m-%d')}.")
                item_data['image_url'] = None

            extracted_items.append(item_data)
        return extracted_items

    def _get_detail_sections(self, root: lxml.html.HtmlElement) -> Dict[str, Optional[lxml.html.HtmlElement]]:
        """
        Parses the detail sections (Infos, Cast, Crew) from the HTML root once
        and returns them in a dictionary for efficient access.

        :param root: The lxml HTML root element.
        :type root: lxml.html.HtmlElement
        :returns: A dictionary mapping section headlines to their corresponding dl elements.
        :rtype: Dict[str, Optional[lxml.html.HtmlElement]]
        """
        sections: Dict[str, Optional[lxml.html.HtmlElement]] = {
            'Infos': None,
            'Cast': None,
            'Crew': None
        }
        for container_elem in self.base_detail_list_container(root):
            headline_p_elements = self.selector_headline_p(container_elem)
            if headline_p_elements and headline_p_elements[0].text_content():
                headline = headline_p_elements[0].text_content().strip()
                if headline in sections:
                    dl_elements = self.selector_dl(container_elem)
                    if dl_elements:
                        sections[headline] = dl_elements[0]
        return sections

    def _extract_description(self, root: lxml.html.HtmlElement, detail_data: Dict[str, Any]) -> None:
        """
        Extracts short and long descriptions from the HTML root.

        :param root: The lxml HTML root element.
        :type root: lxml.html.HtmlElement
        :param detail_data: The dictionary to update with extracted data.
        :type detail_data: Dict[str, Any]
        """
        description_section: List[lxml.html.HtmlElement] = self.selector_description_section(root)
        if description_section:
            short_desc_tag: List[lxml.html.HtmlElement] = self.selector_short_desc_tag(description_section[0])
            if short_desc_tag and short_desc_tag[0].text_content():
                detail_data['shortdescription'] = _sanitize_xml_string(short_desc_tag[0].text_content().strip())

            long_desc_paragraphs: List[lxml.html.HtmlElement] = self.selector_long_desc_paragraphs(description_section[0])
            if long_desc_paragraphs:
                cleaned_paragraphs = [p.text_content().strip() for p in long_desc_paragraphs if p.text_content().strip()]
                if cleaned_paragraphs:
                    detail_data['longdescription'] = _sanitize_xml_string("\n".join(cleaned_paragraphs))

    def _extract_genre_info(self, root: lxml.html.HtmlElement, detail_data: Dict[str, Any], detail_sections: Dict[str, Optional[lxml.html.HtmlElement]]) -> None:
        """
        Extracts genre information from the HTML root using a data-driven approach.

        :param root: The lxml HTML root element.
        :type root: lxml.html.HtmlElement
        :param detail_data: The dictionary to update with extracted data.
        :type detail_data: Dict[str, Any]
        :param detail_sections: Pre-parsed dictionary of detail sections.
        :type detail_sections: Dict[str, Optional[lxml.html.HtmlElement]]
        """
        genre_extraction_strategies = [
            (self.selector_genre_underline_tag,
             lambda elem: (match.group(1).strip() if (match := self.re_genre_pipe_match.search(elem.text_content().strip())) else elem.text_content().strip())),
            (self.selector_genre_tag_general,
             lambda elem: elem.text_content().strip()),
            (self.selector_genre_time_tag_schedule,
             lambda elem: (match.group(1).strip() if (match := self.re_genre_pipe_match.search(elem.text_content().strip())) else elem.text_content().strip()))
        ]

        infos_dl = detail_sections.get('Infos')
        if infos_dl is not None:
            for dt_element in self.selector_dt(infos_dl):
                if dt_element.text_content().strip() == "Genre":
                    genre_dd: Optional[lxml.html.HtmlElement] = dt_element.getnext()
                    if genre_dd and genre_dd.text_content():
                        detail_data['genre'] = _sanitize_xml_string(genre_dd.text_content().strip())
                        return

        for selector, processor in genre_extraction_strategies:
            if detail_data.get('genre'):
                break
            
            elements: List[lxml.html.HtmlElement] = selector(root)
            if elements and elements[0].text_content():
                extracted_genre = processor(elements[0])
                if extracted_genre:
                    detail_data['genre'] = _sanitize_xml_string(extracted_genre)
                    break

    def _extract_general_info(self, root: lxml.html.HtmlElement, detail_data: Dict[str, Any], detail_sections: Dict[str, Optional[lxml.html.HtmlElement]]) -> None:
        """
        Extracts general information (original title, country, year, duration, parental rating)
        from the HTML root.

        :param root: The lxml HTML root element.
        :type root: lxml.html.HtmlElement
        :param detail_data: The dictionary to update with extracted data.
        :type detail_data: Dict[str, Any]
        :param detail_sections: Pre-parsed dictionary of detail sections.
        :type detail_sections: Dict[str, Optional[lxml.html.HtmlElement]]
        """
        infos_dl = detail_sections.get('Infos')
        if infos_dl is not None:
            mapping = {
                'Originaltitel': 'original_title',
                'Land': 'country',
                'Jahr': 'year',
                'Länge': 'duration',
                'FSK': 'parentalrating'
            }
            self._extract_dl_data(infos_dl, mapping, detail_data)

    def _extract_cast_info(self, root: lxml.html.HtmlElement, detail_data: Dict[str, Any], detail_sections: Dict[str, Optional[lxml.html.HtmlElement]]) -> None:
        """
        Extracts cast information from the HTML root.

        :param root: The lxml HTML root element.
        :type root: lxml.html.HtmlElement
        :param detail_data: The dictionary to update with extracted data.
        :type detail_data: Dict[str, Any]
        :param detail_sections: Pre-parsed dictionary of detail sections.
        :type detail_sections: Dict[str, Optional[lxml.html.HtmlElement]]
        """
        cast_dl = detail_sections.get('Cast')
        if cast_dl is not None:
            cast_list: List[str] = []
            for dt_tag in self.selector_dt(cast_dl):
                role: str = dt_tag.text_content().strip()
                actor_dd_tag: Optional[lxml.html.HtmlElement] = dt_tag.getnext()
                if actor_dd_tag is not None and actor_dd_tag.text_content():
                    actor_name: str = actor_dd_tag.text_content().strip()
                    if actor_name:
                        cast_list.append(_sanitize_xml_string(f"{actor_name} ({role})" if role else actor_name) or "")
            detail_data['cast'] = ", ".join(cast_list) if cast_list else None


    def _extract_crew_info(self, root: lxml.html.HtmlElement, detail_data: Dict[str, Any], detail_sections: Dict[str, Optional[lxml.html.HtmlElement]]) -> None:
        """
        Extracts crew information (director, screenplay, camera) from the HTML root.

        :param root: The lxml HTML root element.
        :type root: lxml.html.HtmlElement
        :param detail_data: The dictionary to update with extracted data.
        :type detail_data: Dict[str, Any]
        :param detail_sections: Pre-parsed dictionary of detail sections.
        :type detail_sections: Dict[str, Optional[lxml.html.HtmlElement]]
        """
        crew_dl = detail_sections.get('Crew')
        if crew_dl is not None:
            mapping = {
                'Regie': 'director',
                'Drehbuch': 'screenplay',
                'Kamera': 'camera'
            }
            self._extract_dl_data(crew_dl, mapping, detail_data)

    def _extract_rating_info(self, root: lxml.html.HtmlElement, detail_data: Dict[str, Any]) -> None:
        """
        Extracts rating information (numerical, text, detailed, IMDB) from the HTML root.

        :param root: The lxml HTML root element.
        :type root: lxml.html.HtmlElement
        :param detail_data: The dictionary to update with extracted data.
        :type detail_data: Dict[str, Any]
        """
        content_rating_section: List[lxml.html.HtmlElement] = self.selector_content_rating_section(root)
        if content_rating_section:
            num_rating_div: List[lxml.html.HtmlElement] = self.selector_num_rating_div(content_rating_section[0])
            if num_rating_div and num_rating_div[0].get('class'):
                class_attr: List[str] = num_rating_div[0].get('class', '').split()
                rating_class_match: Optional[re.Match[str]] = self.re_rating_class_match.search(' '.join(class_attr))
                if rating_class_match:
                    try:
                        original_rating = int(rating_class_match.group(1))
                        # Invert numrating: 1->3, 2->2, 3->1
                        detail_data['numrating'] = 4 - original_rating
                    except ValueError:
                        logger.debug(f"Could not parse numerical rating class '{rating_class_match.group(1)}' to integer.")

            txt_rating_tag: List[lxml.html.HtmlElement] = self.selector_txt_rating_tag(content_rating_section[0])
            if txt_rating_tag and txt_rating_tag[0].text_content():
                detail_data['txtrating'] = _sanitize_xml_string(txt_rating_tag[0].text_content().strip())

            rating_list_items: List[lxml.html.HtmlElement] = self.selector_rating_list_items(content_rating_section[0])
            if rating_list_items:
                ratings_parts: List[str] = []
                for item in rating_list_items:
                    label: List[lxml.html.HtmlElement] = self.selector_rating_label(item)
                    rating_span: List[lxml.html.HtmlElement] = self.selector_rating_span(item)
                    if label and label[0].text_content() and rating_span and rating_span[0].get('class'):
                        rating_class_match = self.re_rating_class_match.search(' '.join(rating_span[0].get('class', '').split()))
                        if rating_class_match:
                            try:
                                num_dots: int = int(rating_class_match.group(1))
                                stars: str = '*' * num_dots
                                ratings_parts.append(f"{label[0].text_content().strip()} {stars}")
                            except ValueError:
                                logger.debug(f"Could not parse rating dots from class '{rating_class_match.group(1)}'.")
                if ratings_parts:
                    detail_data['rating'] = _sanitize_xml_string(" / ".join(ratings_parts))

            imdb_rating_tag: List[lxml.html.HtmlElement] = self.selector_imdb_rating_tag(content_rating_section[0])
            if imdb_rating_tag and imdb_rating_tag[0].text_content():
                detail_data['imdbrating'] = _sanitize_xml_string(imdb_rating_tag[0].text_content().strip())

            tipp_tag: List[lxml.html.HtmlElement] = self.selector_tipp_tag(content_rating_section[0])
            if tipp_tag and tipp_tag[0].text_content():
                detail_data['tipp'] = _sanitize_xml_string(tipp_tag[0].text_content().strip())

    def _extract_additional_credits(self, root: lxml.html.HtmlElement, detail_data: Dict[str, Any]) -> None:
        """
        Extracts additional credit information like presenters, commentators, and guests.

        :param root: The lxml HTML root element.
        :type root: lxml.html.HtmlElement
        :param detail_data: The dictionary to update with extracted data.
        :type detail_data: Dict[str, Any]
        """
        detail_data['presenter'] = []
        detail_data['commentator'] = []
        detail_data['guest'] = []

        broadcast_guests_container: List[lxml.html.HtmlElement] = self.selector_broadcast_guests_container(root)
        if not broadcast_guests_container:
            return

        for guests_ul in self.selector_guests_list(broadcast_guests_container[0]):
            title_li: List[lxml.html.HtmlElement] = self.selector_guests_title(guests_ul)
            if not title_li or not title_li[0].text_content():
                continue

            role_title = title_li[0].text_content().strip()
            names = [
                _sanitize_xml_string(li.text_content().strip())
                for li in self.selector_guest_names(guests_ul)
                if li.text_content().strip()
            ]

            if "Moderation:" in role_title:
                detail_data['presenter'].extend(names)
            elif "Kommentiert von:" in role_title:
                detail_data['commentator'].extend(names)
            elif "Mit dabei:" in role_title or "Rateteam:" in role_title:
                detail_data['guest'].extend(names)
            else:
                logger.debug(f"Unknown credit role found: {role_title}")

    def parse_program_details(self, html_content: str, program_item_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Parses the detail page of a program to extract additional information.

        :param html_content: The HTML content of the program detail page.
        :type html_content: str
        :param program_item_data: The dictionary containing existing program data to be updated.
        :type program_item_data: Dict[str, Any]
        :returns: The updated dictionary containing detailed program information.
        :rtype: Dict[str, Any]
        """
        detail_data = TvsLeanScraper._DETAIL_DATA_TEMPLATE.copy()
        detail_data.update(program_item_data)

        root: Optional[lxml.html.HtmlElement] = self._parse_html_root(html_content, f"program details HTML for '{program_item_data.get('title', 'N/A')}'")
        if root is None:
            return program_item_data

        adsc_sparte_match: Optional[re.Match[str]] = self.re_adsc_sparte_match.search(html_content)
        if adsc_sparte_match:
            detail_data['keyword'] = _sanitize_xml_string(adsc_sparte_match.group(1).strip())

        detail_sections = self._get_detail_sections(root)

        self._extract_description(root, detail_data)
        self._extract_genre_info(root, detail_data, detail_sections)
        self._extract_general_info(root, detail_data, detail_sections)
        self._extract_cast_info(root, detail_data, detail_sections)
        self._extract_crew_info(root, detail_data, detail_sections)
        self._extract_rating_info(root, detail_data)
        self._extract_additional_credits(root, detail_data)

        subheadline_tag: List[lxml.html.HtmlElement] = self.selector_subheadline(root)
        if subheadline_tag and subheadline_tag[0].text_content():
            potential_subtitle = _sanitize_xml_string(subheadline_tag[0].text_content().strip())

            if potential_subtitle and detail_data.get('title'):
                title = detail_data['title'].strip()

                normalized_title_for_comparison = self._normalize_string_for_comparison(title)
                normalized_subtitle_temp = self._normalize_string_for_comparison("mehr zu ")

                if normalized_subtitle_temp.startswith(self._normalize_string_for_comparison("mehr zu ")):
                    subtitle_without_prefix = potential_subtitle[len("mehr zu "):].strip()
                    normalized_subtitle_without_prefix = self._normalize_string_for_comparison(subtitle_without_prefix)

                    if normalized_title_for_comparison == normalized_subtitle_without_prefix:
                        logger.debug(f"Discarding redundant subtitle '{potential_subtitle}' for title '{title}'.")
                        detail_data['subtitle'] = None
                    else:
                        detail_data['subtitle'] = potential_subtitle
                else:
                    detail_data['subtitle'] = potential_subtitle
        else:
            logger.debug("Subtitle tag (h2.broadcast-info) not found or empty.")

        season_episode_tag: List[lxml.html.HtmlElement] = self.selector_season_episode(root)
        if season_episode_tag and season_episode_tag[0].text_content():
            episode_text = season_episode_tag[0].text_content().strip()
            episode_match = self.re_season_episode_match.search(episode_text)
            if episode_match:
                try:
                    season = int(episode_match.group(1))
                    episode = int(episode_match.group(2))
                    detail_data['season_num'] = season
                    detail_data['episode_num'] = episode
                except ValueError:
                    logger.debug(f"Could not parse season/episode numbers from '{episode_text}'.")
            else:
                logger.debug(f"Could not parse season/episode from '{episode_text}'.")

        return detail_data


class TvsLeanScraper:
    """
    Encapsulates the scraping logic for TVSpielfilm.de.
    Handles fetching HTML, parsing channel lists and program schedules,
    extracting detailed program information, and managing caching and retries.
    """
    _DETAIL_DATA_TEMPLATE: Dict[str, Any] = {
        'shortdescription': None, 'longdescription': None, 'genre': None,
        'original_title': None, 'country': None, 'year': None, 'duration': None,
        'parentalrating': None, 'cast': None, 'director': None, 'screenplay': None,
        'camera': None, 'numrating': None, 'rating': None, 'txtrating': None,
        'imdbrating': None, 'keyword': None,
        'subtitle': None,
        'episode_num': None,
        'season_num': None,
        'presenter': [],
        'commentator': [],
        'guest': [],
        'tipp': None,
        'detail_etag': None,
        'detail_last_modified': None,
        'detail_content_length': None,
        'detail_cached_at': None,
        'image_check_final_url': None,
        'image_check_status': None
    }

    def __init__(self, channel_ids: Optional[str] = None, days: Optional[int] = None, image_size: Optional[str] = None, check_image: Optional[bool] = None, start_date_obj: Optional[datetime.date] = None,
                 cache_dir_path: Optional[str] = None, max_workers: int = DEFAULT_MAX_WORKERS, max_retries: int = DEFAULT_MAX_RETRIES,
                 channel_ids_file: Optional[str] = None, min_request_delay: float = DEFAULT_MIN_REQUEST_DELAY,
                 max_concurrent_requests: int = DEFAULT_MAX_CONCURRENT_REQUESTS,
                 max_schedule_retries: int = DEFAULT_MAX_SCHEDULE_RETRIES,
                 disable_cache: bool = DEFAULT_CACHE_DISABLE, cache_ttl: int = DEFAULT_CACHE_TTL_SECONDS,
                 keep_past_cache: bool = False, cache_clear: bool = False, xmltv_timezone: Optional[str] = None,
                 cache_validation_tolerance: int = DEFAULT_CACHE_VALIDATION_TOLERANCE,
                 http_timeout: int = DEFAULT_HTTP_TIMEOUT):
        """
        Initializes the TvsLeanScraper instance.

        :param channel_ids: Comma-separated string of channel IDs to scrape.
        :type channel_ids: Optional[str]
        :param days: Number of days to scrape.
        :type days: Optional[int]
        :param image_size: Desired image size ("150", "300" or "600").
        :type image_size: Optional[str]
        :param check_image: Boolean to enable/disable image URL validity checks.
        :type check_image: Optional[bool]
        :param start_date_obj: Specific date to scrape as a datetime.date object.
        :type start_date_obj: Optional[datetime.date]
        :param cache_dir_path: Custom directory for cache files.
        :type cache_dir_path: Optional[str]
        :param max_workers: Maximum number of concurrent workers.
        :type max_workers: int
        :param max_retries: Maximum number of retries for failed HTTP requests.
        :type max_retries: int
        :param channel_ids_file: Path to a file containing comma-separated channel IDs.
        :type channel_ids_file: Optional[str]
        :param min_request_delay: Minimum delay in seconds between HTTP requests.
        :type min_request_delay: float
        :param max_concurrent_requests: Maximum number of concurrent HTTP requests allowed.
                                        This acts as a global rate limiter.
        :type max_concurrent_requests: int
        :param max_schedule_retries: Maximum retries for application-level schedule parsing errors.
        :type max_schedule_retries: int
        :param disable_cache: If True, disables processed JSON data caching.
        :type disable_cache: bool
        :param cache_ttl: Cache Time To Live in seconds.
        :type cache_ttl: int
        :param keep_past_cache: If True, prevents automatic deletion of past days' cache files.
        :type keep_past_cache: bool
        :param cache_clear: If True, clears all caches before starting.
        :type cache_clear: bool
        :param xmltv_timezone: Specifies the timezone for XMLTV output.
        :type xmltv_timezone: Optional[str]
        :param cache_validation_tolerance: Tolerance in bytes for content-length comparison when ETag/Last-Modified fails to return 304.
        :type cache_validation_tolerance: int
        :param http_timeout: Timeout for HTTP requests in seconds.
        :type http_timeout: int
        :raises NotADirectoryError: If the specified cache path exists but is not a directory.
        """
        self.target_sourceids: Optional[set[str]] = None
        if channel_ids_file:
            try:
                with open(channel_ids_file, 'r', encoding='utf-8') as f:
                    file_content: str = f.read().strip()
                if file_content:
                    self.target_sourceids = {cid.strip().lower() for cid in file_content.split(',') if cid.strip()}
                    logger.info(f"Target channels from file '{channel_ids_file}': {self.target_sourceids}")
                else:
                    logger.warning(f"Channel IDs file '{channel_ids_file}' is empty. All found channels will be crawled.")
            except FileNotFoundError:
                logger.error(f"Channel IDs file '{channel_ids_file}' not found. All found channels will be crawled.")
            except Exception as e:
                logger.error(f"Error reading channel IDs from file '{channel_ids_file}': {e}. All found channels will be crawled.")
        elif channel_ids:
            if isinstance(channel_ids, str):
                self.target_sourceids = {cid.strip().lower() for cid in channel_ids.split(',') if cid.strip()}
                logger.info(f"Target channels from argument: {self.target_sourceids}")
            else:
                logger.warning(f"Invalid ID type for channel_ids: {type(channel_ids)}. Expected string. Crawling all found channels.")
        else:
            logger.debug("No specific channels targeted. Crawling all found channels.")

        self.is_specific_date_set: bool = start_date_obj is not None
        self.start_date: datetime.date = start_date_obj if start_date_obj else datetime.now().date()
        
        self.days_to_scrape = int(days) if days is not None else DEFAULT_DAYS
        if self.is_specific_date_set:
            self.days_to_scrape = 1

        self.image_size: str = image_size if image_size in ["150", "300", "600"] else DEFAULT_IMAGE_SIZE
        self.check_image: bool = bool(check_image) if check_image is not None else DEFAULT_IMAGE_CHECK
        if self.check_image:
            logger.debug("Image URL validity checks (--img-check) are ENABLED. This may increase scraping time.")
        else:
            logger.debug("Image URL validity checks (--img-check) are DISABLED.")

        self.cache_dir: str = cache_dir_path if cache_dir_path else os.path.join(tempfile.gettempdir(), DEFAULT_CACHE_SUBDIRECTORY)

        if os.path.exists(self.cache_dir):
            if not os.path.isdir(self.cache_dir):
                logger.error(f"Cache path '{self.cache_dir}' exists but is not a directory. Please resolve this conflict.")
                raise NotADirectoryError(f"Cache path '{self.cache_dir}' exists but is not a directory.")
        else:
            os.makedirs(self.cache_dir, exist_ok=True)

        self.user_agents: List[str] = USER_AGENTS

        self.max_workers: int = max_workers
        self.pool_size: int = max(self.max_workers, 1) * 4

        self.retry_strategy: Retry = Retry(
            total=max_retries,
            backoff_factor=RETRY_BACKOFF_FACTOR,
            status_forcelist=RETRY_STATUS_FORCELIST,
            allowed_methods=["GET", "HEAD"],
            raise_on_status=False,
            respect_retry_after_header=True
        )
        self.session = requests.Session()
        adapter = HTTPAdapter(
            max_retries=self.retry_strategy,
            pool_connections=self.pool_size,
            pool_maxsize=self.pool_size,
            pool_block=True
        )
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        logger.debug("Created a single requests.Session for the scraper instance.")
        logger.debug(f"Main session retry strategy configured: Max retries={max_retries}, Backoff factor={RETRY_BACKOFF_FACTOR}, Retrying on status codes={RETRY_STATUS_FORCELIST}, respecting Retry-After header.")
        logger.debug(f"HTTPAdapter connection pool size set to: {self.pool_size}")

        self.min_request_delay: float = min_request_delay
        self.max_schedule_retries: int = max_schedule_retries
        self.request_semaphore = threading.BoundedSemaphore(value=max_concurrent_requests)
        logger.info(f"Max concurrent HTTP requests limited to: {max_concurrent_requests}.")
        self.last_request_time = 0.0
        self.last_request_time_lock = threading.Lock()

        self.enable_cache: bool = not disable_cache
        self.cache_ttl: int = cache_ttl
        self.processed_data_cache_base_dir: str = os.path.join(self.cache_dir, CACHE_PROCESSED_DATA_SUBDIRECTORY)
        self.keep_past_cache: bool = bool(keep_past_cache)
        self.cache_clear_requested: bool = cache_clear
        self.cache_validation_tolerance: int = cache_validation_tolerance
        self.http_timeout: int = http_timeout

        if self.enable_cache:
            os.makedirs(self.processed_data_cache_base_dir, exist_ok=True)
            logger.debug(f"Processed data cache enabled. Path: {self.processed_data_cache_base_dir}, TTL: {self.cache_ttl}s.")
            logger.debug("Conditional GET (ETag/Last-Modified/304) for cache consistency is ENABLED by default.")
            logger.debug(f"Content-Length tolerance for cache validation set to {self.cache_validation_tolerance} bytes.")
            if self.keep_past_cache:
                logger.debug("Keeping past days' cache files is ENABLED (--keep-past-cache).")
            else:
                logger.debug("Deleting past days' cache files is ENABLED by default (no --keep-past-cache).")
        else:
            logger.debug("Processed data cache disabled.")

        if self.cache_clear_requested:
            logger.info("Explicit --cache-clear requested. All caches will be invalidated/ignored.")

        self.stop_event = threading.Event()
        self.html_parser = TvsHtmlParser()
        self.xmltv_timezone: str = xmltv_timezone if xmltv_timezone else 'Europe/Berlin'

    def _build_schedule_url(self, channel_url: str, current_date: datetime.date) -> str:
        """
        Constructs the schedule URL for a given channel and date.

        :param channel_url: Base URL of the channel.
        :type channel_url: str
        :param current_date: The date for which to build the schedule URL.
        :type current_date: datetime.date
        :returns: The complete schedule URL.
        :rtype: str
        """
        date_str: str = current_date.strftime('%Y-%m-%d')
        return f"{channel_url}?date={date_str}"

    def _build_request_headers(self, etag: Optional[str] = None, last_modified: Optional[str] = None) -> Dict[str, str]:
        """
        Builds the HTTP headers dictionary for a request.

        :param etag: The ETag from a previous response, for conditional GET.
        :type etag: Optional[str]
        :param last_modified: The Last-Modified header from a previous response, for conditional GET.
        :type last_modified: Optional[str]
        :returns: A dictionary of HTTP headers.
        :rtype: Dict[str, str]
        """
        headers: Dict[str, str] = {'User-Agent': random.choice(self.user_agents)}
        if etag:
            headers['If-None-Match'] = etag
        if last_modified:
            headers['If-Modified-Since'] = last_modified
        return headers

    def _execute_http_request(self, method: str, url: str, headers: Dict[str, str], allow_redirects: bool, timeout: int) -> Optional[requests.Response]:
        """
        Internal function to make HTTP requests and handle RequestExceptions.
        Contains common logic for stop event checks, URL validation, and request execution.
        Applies rate limiting using a semaphore and minimum delay.

        :param method: HTTP method ('get' or 'head').
        :type method: str
        :param url: The URL to fetch.
        :type url: str
        :param headers: Dictionary of HTTP headers.
        :type headers: Dict[str, str]
        :param allow_redirects: Whether to follow HTTP redirects.
        :type allow_redirects: bool
        :param timeout: Request timeout in seconds.
        :type timeout: int
        :returns: The requests.Response object on success, None on failure.
        :rtype: Optional[requests.Response]
        """
        if self.stop_event.is_set():
            logger.debug(f"Stop event set. Aborting {method.upper()} for {url}.")
            return None

        parsed_url = urlparse(url)
        if not all([parsed_url.scheme, parsed_url.netloc]) or parsed_url.scheme not in ("http", "https"):
            logger.warning(f"Invalid or unsupported URL scheme: {url}. Skipping request.")
            return None

        with self.request_semaphore:
            with self.last_request_time_lock:
                elapsed = time.time() - self.last_request_time
                if elapsed < self.min_request_delay:
                    sleep_time = self.min_request_delay - elapsed
                    time.sleep(sleep_time)
                self.last_request_time = time.time()

            try:
                if method == 'get':
                    response: requests.Response = self.session.get(url, headers=headers, allow_redirects=allow_redirects, timeout=timeout)
                elif method == 'head':
                    response: requests.Response = self.session.head(url, headers=headers, allow_redirects=allow_redirects, timeout=timeout)
                else:
                    logger.error(f"Unsupported HTTP method: {method}")
                    return None

                if self.stop_event.is_set():
                    logger.debug(f"Stop event set during {method.upper()} for {url}.")
                    return None

                response.raise_for_status()
                return response
            except requests.exceptions.Timeout:
                logger.error(f"Request timed out for {url} ({method.upper()}) after {timeout} seconds.")
                return None
            except requests.exceptions.ConnectionError as e:
                logger.error(f"Connection error for {url} ({method.upper()}): {e}")
                return None
            except requests.exceptions.HTTPError as e:
                logger.error(f"HTTP error for {url} ({method.upper()}): {e}")
                return None
            except requests.exceptions.RequestException as e:
                logger.error(f"General request failed for {url} ({method.upper()}): {e}")
                return None

    @functools.lru_cache(maxsize=256)
    def _make_request_cached_wrapper(self, method: str, url: str, headers_tuple: Tuple[Tuple[str, str], ...], allow_redirects: bool, timeout: int) -> Optional[requests.Response]:
        """
        A cached wrapper for _execute_http_request. This method is decorated with lru_cache.
        Headers are passed as a hashable tuple of tuples.

        :param method: HTTP method ('get' or 'head').
        :type method: str
        :param url: The URL to fetch.
        :type url: str
        :param headers_tuple: Tuple of (header_name, header_value) tuples for headers.
        :type headers_tuple: Tuple[Tuple[str, str], ...]
        :param allow_redirects: Whether to follow HTTP redirects.
        :type allow_redirects: bool
        :param timeout: Request timeout in seconds.
        :type timeout: int
        :returns: The requests.Response object on success, None on failure.
        :rtype: Optional[requests.Response]
        """
        headers = dict(headers_tuple)
        return self._execute_http_request(method, url, headers, allow_redirects, timeout)

    def fetch_url(self, url: str, etag: Optional[str] = None, last_modified: Optional[str] = None, cached_content_length: Optional[int] = None, allow_redirects: bool = True) -> Tuple[Optional[str], Optional[str], Optional[str], Optional[int]]:
        """
        Sends an HTTP GET request to the given URL using the configured session.
        Handles HTTP status codes, timeouts, and retries. Applies a delay only for
        live (non-cached) requests to be polite. Implements Conditional GET using ETag and Last-Modified headers.
        If a 304 Not Modified is not received, it checks content-length with a tolerance.

        :param url: The URL to fetch.
        :type url: str
        :param etag: The ETag from a previous response, for conditional GET.
        :type etag: Optional[str]
        :param last_modified: The Last-Modified header from a previous response, for conditional GET.
        :type last_modified: Optional[str]
        :param cached_content_length: The Content-Length from the last cached response.
        :type cached_content_length: Optional[int]
        :param allow_redirects: Whether to follow HTTP redirects. Defaults to True.
        :type allow_redirects: bool
        :returns: A tuple (response_content, new_etag, new_last_modified, fetched_content_length).
                  response_content is None if 304 Not Modified or if content is semantically identical within tolerance.
                  fetched_content_length is for the *newly fetched* content.
                  Returns (None, None, None, None) if request fails.
        :rtype: Tuple[Optional[str], Optional[str], Optional[str], Optional[int]]
        """
        if self.stop_event.is_set():
            logger.debug(f"Stop event set. Aborting fetch for {url}.")
            return None, None, None, None

        headers = self._build_request_headers(etag, last_modified)

        if self.cache_clear_requested:
            logger.debug(f"Performing full GET for {url} (explicit --cache-clear requested).")
            response = self._execute_http_request('get', url, headers, allow_redirects, self.http_timeout)
            if response is None:
                return None, None, None, None

            new_etag: Optional[str] = response.headers.get('ETag')
            new_last_modified: Optional[str] = response.headers.get('Last-Modified')
            fetched_content_length: int = len(response.content)

            logger.debug(f"Fetched live: {url} (Status: {response.status_code}). ETag: {new_etag}, Last-Modified: {new_last_modified}, Content-Length: {fetched_content_length}")
            return response.text, new_etag, new_last_modified, fetched_content_length

        headers_tuple = tuple(sorted(headers.items()))
        response = self._make_request_cached_wrapper('get', url, headers_tuple, allow_redirects, self.http_timeout)

        if response is None:
            return None, None, None, None

        new_etag: Optional[str] = response.headers.get('ETag')
        new_last_modified: Optional[str] = response.headers.get('Last-Modified')
        fetched_content_length: int = len(response.content)

        if response.status_code == 304:
            logger.debug(f"Resource {url} not modified (304). Using cached data.")
            return None, etag, last_modified, cached_content_length

        if cached_content_length is not None and \
           abs(fetched_content_length - cached_content_length) <= self.cache_validation_tolerance:
            logger.debug(f"Resource {url} content length is within tolerance ({self.cache_validation_tolerance} bytes). "
                         f"Fetched: {fetched_content_length}, Cached: {cached_content_length}. Using cached data.")
            return None, etag, last_modified, cached_content_length
        else:
            if cached_content_length is not None:
                logger.debug(f"Resource {url} content length differs beyond tolerance. "
                             f"Fetched: {fetched_content_length}, Cached: {cached_content_length}, Tolerance: {self.cache_validation_tolerance}. Fetching new content.")
            elif etag is not None or last_modified is not None:
                logger.debug(f"Resource {url} was not 304 Not Modified despite ETag/Last-Modified. Fetching new content.")
            else:
                logger.debug(f"No cache metadata available for {url}. Performing initial fetch.")
            return response.text, new_etag, new_last_modified, fetched_content_length

    def _load_json_with_lock(self, filepath: str) -> Optional[Dict[str, Any]]:
        """
        Loads JSON data from a file with shared locking.

        :param filepath: The path to the JSON file.
        :type filepath: str
        :returns: The loaded JSON content as a dictionary, or None if loading fails.
        :rtype: Optional[Dict[str, Any]]
        """
        if not os.path.exists(filepath):
            logger.debug(f"Cache file not found at {filepath}.")
            return None
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                portalocker.lock(f, portalocker.LOCK_SH)
                try:
                    data = json.load(f)
                    return data
                except json.JSONDecodeError as e:
                    logger.warning(f"Error decoding cache: {e}. Deleting corrupted file: {filepath}")
                    f.seek(0)
                    f.truncate()
                    if os.path.isfile(filepath):
                        os.remove(filepath)
                    return None
                finally:
                    portalocker.unlock(f)
        except portalocker.LockException as e:
            logger.warning(f"Could not acquire lock for cache file {filepath}: {e}. Skipping cache load.")
            return None
        except IOError as e:
            logger.warning(f"IOError loading cache from {filepath}: {e}.")
            return None
        except Exception as e:
            logger.error(f"Unexpected error in _load_json_with_lock for {filepath}: {e}")
            return None

    def _save_json_with_lock(self, filepath: str, data: Dict[str, Any]) -> None:
        """
        Saves JSON data to a file using an atomic write operation with a temporary file and exclusive locking.

        :param filepath: The path to the target JSON file.
        :type filepath: str
        :param data: The dictionary data to save.
        :type data: Dict[str, Any]
        :returns: None
        :rtype: None
        """
        temp_filepath: str = f"{filepath}.tmp.{os.getpid()}"
        try:
            with open(temp_filepath, 'w+', encoding='utf-8') as f:
                portalocker.lock(f, portalocker.LOCK_EX)
                try:
                    json.dump(data, f, ensure_ascii=False, indent=4)
                    f.flush()
                    os.fsync(f.fileno())
                finally:
                    portalocker.unlock(f)
            os.replace(temp_filepath, filepath)
            logger.debug(f"Saved data to cache: {filepath}")
        except portalocker.LockException as e:
            logger.warning(f"Could not acquire lock for temporary cache file during save: {e}. Skipping cache save.")
            if os.path.exists(temp_filepath):
                os.remove(temp_filepath)
        except IOError as e:
            logger.error(f"Error saving cache to {filepath} (IOError): {e}.")
            if os.path.exists(temp_filepath):
                os.remove(temp_filepath)
        except Exception as e:
            logger.error(f"Unexpected error saving cache to {filepath}: {e}")
            if os.path.exists(temp_filepath):
                os.remove(temp_filepath)

    def _is_cache_fresh(self, cached_at_str: Optional[str]) -> bool:
        """
        Checks if a cache entry is still fresh based on its timestamp and the configured TTL.

        :param cached_at_str: The ISO formatted UTC timestamp string when the cache was created.
        :type cached_at_str: Optional[str]
        :returns: True if the cache is fresh, False otherwise.
        :rtype: bool
        """
        if not cached_at_str:
            return False
        try:
            cached_at: datetime = datetime.fromisoformat(cached_at_str)
            if cached_at.tzinfo is None:
                cached_at = cached_at.replace(tzinfo=timezone.utc)
            current_time_utc: datetime = datetime.now(timezone.utc)
            return (current_time_utc - cached_at).total_seconds() < self.cache_ttl
        except ValueError as e:
            logger.warning(f"Invalid timestamp in cache: {cached_at_str}. Error: {e}. Treating cache as stale.")
            return False

    def _get_channel_list(self) -> List[Dict[str, str]]:
        """
        Fetches and parses the list of channels from TVSpielfilm.de.
        Utilizes caching with ETag/Last-Modified for efficient fetching.

        :returns: A list of dictionaries, each containing 'name', 'url', and 'source_id' for a channel.
        :rtype: List[Dict[str, str]]
        """
        if self.stop_event.is_set():
            logger.debug("Stop event set. Aborting _get_channel_list.")
            return []

        channels: List[Dict[str, str]] = []
        
        cached_data_from_file: Optional[Dict[str, Any]] = None
        cached_channels_list: Optional[List[Dict[str, Any]]] = None
        cached_etag: Optional[str] = None
        cached_last_modified = None
        cached_content_length: Optional[int] = None
        is_cache_fresh: bool = False

        if self.enable_cache and not self.cache_clear_requested:
            filepath = self._get_cache_channel_list_filepath()
            cached_data_from_file = self._load_json_with_lock(filepath)

            if cached_data_from_file:
                cached_channels_list = cached_data_from_file.get('data')
                cached_etag = cached_data_from_file.get('etag')
                cached_last_modified = cached_data_from_file.get('last_modified')
                cached_at_str = cached_data_from_file.get('cached_at')
                cached_content_length = cached_data_from_file.get('content_length')
                if cached_content_length is None:
                    cached_content_length = 0

                is_cache_fresh = self._is_cache_fresh(cached_at_str)

                if is_cache_fresh and cached_channels_list is not None:
                    channels = cached_channels_list
                    logger.debug("Using fresh channel list from cache.")
                    return channels
                elif cached_channels_list is not None:
                    logger.debug("Channel list cache is stale. Attempting conditional fetch.")
                else:
                    logger.debug("Channel list cache found but data is malformed. Performing full fetch.")
            else:
                logger.debug("Channel list cache not found or unreadable. Performing full fetch.")
        else:
            if self.cache_clear_requested:
                logger.debug("Cache clear requested. Bypassing channel list cache.")
            elif not self.enable_cache:
                logger.debug("Cache disabled. Bypassing channel list cache.")

        logger.info(f"Fetching channel list from {CHANNEL_LIST_URL}")
        response_content, temp_new_etag, temp_new_last_modified, temp_fetched_content_length = self.fetch_url(
            CHANNEL_LIST_URL,
            etag=cached_etag,
            last_modified=cached_last_modified,
            cached_content_length=cached_content_length
        )

        if response_content is not None:
            logger.debug("Channel list HTML content was modified or fetched for the first time. Parsing new data.")
            channels = self.html_parser.parse_channel_list(response_content)
            if self.enable_cache:
                cache_content: Dict[str, Any] = {
                    'data': channels,
                    'etag': temp_new_etag,
                    'last_modified': temp_new_last_modified,
                    'cached_at': datetime.now(timezone.utc).isoformat(),
                    'content_length': temp_fetched_content_length,
                }
                self._save_json_with_lock(self._get_cache_channel_list_filepath(), cache_content)
        elif cached_channels_list is not None:
            logger.debug("Channel list HTML content not modified (304) or semantically identical. Using stale cache.")
            channels = cached_channels_list
        else:
            logger.error(f"Failed to fetch channel list from {CHANNEL_LIST_URL} and no cache available.")

        return channels

    def _get_cache_channel_list_filepath(self) -> str:
        """
        Constructs the file path for the channel list JSON cache.

        :returns: The full path to the JSON file.
        :rtype: str
        """
        return os.path.join(self.processed_data_cache_base_dir, CACHE_CHANNEL_LIST_FILE)

    def _get_daily_cache_filepath(self, channel_id: str, date: datetime.date) -> str:
        """
        Constructs the file path for the daily schedule JSON cache.

        :param channel_id: The ID of the channel.
        :type channel_id: str
        :param date: The date of the schedule.
        :type date: datetime.date
        :returns: The full path to the JSON file for the daily schedule.
        :rtype: str
        """
        channel_cache_dir = os.path.join(self.processed_data_cache_base_dir, channel_id.lower())
        os.makedirs(channel_cache_dir, exist_ok=True)
        return os.path.join(channel_cache_dir, date.strftime('%Y%m%d') + '.json')

    def _load_daily_json_cache(self, channel_id: str, date: datetime.date) -> Tuple[Optional[List[Dict[str, Any]]], Optional[str], Optional[str], Optional[int], Optional[str], bool, Optional[str]]:
        """
        Loads processed JSON data from the daily data cache, along with ETag, Last-Modified, Content-Length,
        and the timezone it was cached with.
        Checks if the data is fresh based on its internal timestamp and TTL.
        Also deletes cache files for past days (if --keep-past-cache is not set) and for dates
        beyond MAX_DAYS_TO_SCRAPE from the current date.

        :param channel_id: The ID of the channel.
        :type channel_id: str
        :param date: The date of the schedule.
        :type date: datetime.date
        :returns: A tuple (data, etag, last_modified, content_length, cached_xmltv_timezone, is_fresh, cached_at_str).
                  data is None if file not found or corrupted.
                  is_fresh is True if the data is within TTL.
                  cached_at_str is the original timestamp from the cache file.
        :rtype: Tuple[Optional[List[Dict[str, Any]]], Optional[str], Optional[str], Optional[int], Optional[str], bool, Optional[str]]
        """
        if self.stop_event.is_set():
            logger.debug(f"Stop event set. Aborting cache load for {channel_id} on {date.strftime('%Y-%m-%d')}.")
            return None, None, None, None, None, False, None

        filepath: str = self._get_daily_cache_filepath(channel_id, date)

        if not self.keep_past_cache and date < (datetime.now().date() - timedelta(days=1)):
            logger.debug(f"Deleting stale cache file for past day {date.strftime('%Y-%m-%d')} for channel {channel_id} as --keep-past-cache is not set.")
            try:
                os.remove(filepath)
            except OSError as e:
                logger.error(f"Error deleting cache file {filepath}: {e}")
            return None, None, None, None, None, False, None

        max_relevant_date = datetime.now().date() + timedelta(days=MAX_DAYS_TO_SCRAPE - 1)
        if date > max_relevant_date:
            logger.debug(f"Deleting outdated cache file for {date.strftime('%Y-%m-%d')} for channel {channel_id} (beyond max relevant date).")
            try:
                os.remove(filepath)
            except OSError as e:
                logger.error(f"Error deleting cache file {filepath}: {e}")
            return None, None, None, None, None, False, None

        cached_content = self._load_json_with_lock(filepath)
        if not cached_content:
            return None, None, None, None, None, False, None

        data: Optional[List[Dict[str, Any]]] = cached_content.get('data')
        etag: Optional[str] = cached_content.get('etag')
        last_modified: Optional[str] = cached_content.get('last_modified')
        cached_at_str: Optional[str] = cached_content.get('cached_at')
        content_length: Optional[int] = cached_content.get('content_length')
        if content_length is None:
            content_length = 0
        cached_xmltv_timezone: Optional[str] = cached_content.get('xmltv_timezone')

        is_fresh: bool = self._is_cache_fresh(cached_at_str)

        if is_fresh:
            logger.debug(f"Loaded schedule for {channel_id} on {date.strftime('%Y-%m-%d')} from daily cache (fresh).")
        else:
            logger.debug(f"Daily cache for {channel_id} on {date.strftime('%Y-%m-%d')} is stale. Will attempt conditional fetch.")

        return data, etag, last_modified, content_length, cached_xmltv_timezone, is_fresh, cached_at_str

    def _save_daily_json_cache(self, channel_id: str, date: datetime.date, data: List[Dict[str, Any]], etag: Optional[str] = None, last_modified: Optional[str] = None, content_length: Optional[int] = None, xmltv_timezone_used: Optional[str] = None, cached_at_to_use: Optional[str] = None) -> None:
        """
        Saves processed JSON data to the daily data cache, including HTTP ETag, Last-Modified,
        Content-Length headers, and the timezone used for XMLTV time formatting.

        :param channel_id: The ID of the channel.
        :type channel_id: str
        :param date: The date of the schedule.
        :type date: datetime.date
        :param data: The program data for the day to save.
        :type data: List[Dict[str, Any]]
        :param etag: The ETag header value.
        :type etag: Optional[str]
        :param last_modified: The Last-Modified header value.
        :type last_modified: Optional[str]
        :param content_length: The Content-Length of the fetched content.
        :type content_length: Optional[int]
        :param xmltv_timezone_used: The timezone string used when generating 'starttime_xmltv' and 'endtime_xmltv'.
        :type xmltv_timezone_used: Optional[str]
        :param cached_at_to_use: Optional. If provided, this timestamp will be used for 'cached_at'.
                                 Otherwise, the current UTC time will be used.
        """
        if self.stop_event.is_set():
            logger.debug(f"Stop event set. Aborting cache save for {channel_id} on {date.strftime('%Y-%m-%d')}.")
            return

        filepath: str = self._get_daily_cache_filepath(channel_id, date)
        cache_content: Dict[str, Any] = {
            'data': data,
            'etag': etag,
            'last_modified': last_modified,
            'cached_at': cached_at_to_use if cached_at_to_use else datetime.now(timezone.utc).isoformat(),
            'content_length': content_length,
            'xmltv_timezone': xmltv_timezone_used
        }
        self._save_json_with_lock(filepath, cache_content)

    def _strip_im_parameter_for_cache(self, url: str) -> str:
        """
        Strips the 'im' parameter from a URL for saving to JSON cache.
        This function always removes the 'im' parameter.

        :param url: The original URL.
        :type url: str
        :returns: The URL with 'im' parameter removed.
        :rtype: str
        """
        parsed = urlparse(url)
        query_params = parse_qs(parsed.query)

        if 'im' in query_params:
            new_query_parts = []
            for key, values in query_params.items():
                if key != 'im':
                    for value in values:
                        new_query_parts.append(f"{key}={value}")
            new_query_string = '&'.join(new_query_parts)
            return parsed._replace(query=new_query_string).geturl()
        return url

    def _check_image_url_head_threaded(self, original_image_url: str, item_id_for_log: str) -> Tuple[str, str]:
        """
        Performs a HEAD request to check if an image URL is valid and returns its final URL after redirects.
        Designed to be called in a threaded context.

        :param original_image_url: The original image URL to check.
        :type original_image_url: str
        :param item_id_for_log: A unique identifier for logging purposes for the current item.
        :type item_id_for_log: str
        :returns: A tuple (final_url, status). 'final_url' is the URL after redirects or original if no redirect/error.
                  'status' is 'avail' if OK (200), 'not_avail' if 4xx/5xx, 'redirect' if redirect to placeholder, 'error' for others.
        :rtype: Tuple[str, str]
        """
        if self.stop_event.is_set():
            logger.debug(f"Stop event set. Aborting image check for {item_id_for_log}.")
            return original_image_url, 'error'

        final_url: str = original_image_url
        status: str = 'error'

        try:
            headers = self._build_request_headers()
            headers_tuple = tuple(sorted(headers.items()))
            response = self._make_request_cached_wrapper('head', original_image_url, headers_tuple, allow_redirects=True, timeout=self.http_timeout)

            if response:
                final_url = response.url
                if response.status_code == 200:
                    status = 'avail'
                elif 400 <= response.status_code < 500:
                    status = 'not_avail'
                    logger.debug(f"Image {item_id_for_log} not available (HTTP {response.status_code}): {final_url}")
                elif 500 <= response.status_code < 600:
                    status = 'not_avail'
                    logger.debug(f"Image {item_id_for_log} not available (HTTP {response.status_code}): {final_url}")
                else:
                    status = 'error'
                    logger.debug(f"Image {item_id_for_log} check returned unexpected status {response.status_code}: {final_url}")
            else:
                status = 'error'
                logger.debug(f"Failed HEAD request for image {item_id_for_log}: {original_image_url}")

        except Exception as e:
            status = 'error'
            logger.debug(f"Exception during HEAD request for image {item_id_for_log}: {e}. URL: {original_image_url}")

        if status == 'avail' and (
            'placehold.it' in final_url or
            'placeholder.com' in final_url or
            'dummyimage.com' in final_url or
            'empty.jpg' in final_url.lower() or
            'noimage' in final_url.lower()
        ):
            status = 'redirect'
            logger.info(f"Image {item_id_for_log} redirected to a generic placeholder: {final_url}")

        return final_url, status

    def _fetch_detail_and_image_threaded(self, detail_url: str, program_item_data: Dict[str, Any], original_image_url: Optional[str], item_id_for_log: str) -> Tuple[Dict[str, Any], Optional[str], Optional[str], Optional[int]]:
        """
        Combines detail page parsing and image head check into a single threaded task.
        Performs conditional GET for the detail page.
        This function now respects the cache_ttl for detail pages directly.

        :param detail_url: The URL of the program's detail page.
        :type detail_url: str
        :param program_item_data: The dictionary containing existing program data to be updated.
        :type program_item_data: Dict[str, Any]
        :param original_image_url: The original image URL to check. Can be None if only detail is fetched.
        :type original_image_url: Optional[str]
        :param item_id_for_log: A unique identifier for logging purposes for the current item.
        :type item_id_for_log: str
        :returns: A tuple (detail_data, new_detail_etag, new_detail_last_modified, new_detail_content_length).
                  detail_data is the updated dictionary containing detailed program information and image check results.
        :rtype: Tuple[Dict[str, Any], Optional[str], Optional[str], Optional[int]]
        """
        cached_detail_etag = program_item_data.get('detail_etag')
        cached_detail_last_modified = program_item_data.get('detail_last_modified')
        cached_detail_content_length = program_item_data.get('detail_content_length')
        detail_cached_at_str = program_item_data.get('detail_cached_at')

        is_detail_cache_fresh = self._is_cache_fresh(detail_cached_at_str)

        detail_data = program_item_data.copy()
        new_detail_etag = cached_detail_etag
        new_detail_last_modified = cached_detail_last_modified
        new_detail_content_length = cached_detail_content_length

        detail_data['image_check_final_url'] = None
        detail_data['image_check_status'] = "N/A"

        if self.enable_cache and not self.cache_clear_requested and is_detail_cache_fresh:
            logger.debug(f"Detail page for {item_id_for_log} is fresh in cache. Using existing detail data.")
        else:
            if detail_cached_at_str is None:
                logger.debug(f"No detail cache metadata found for {item_id_for_log}. Performing initial fetch.")
            else:
                logger.debug(f"Detail cache for {item_id_for_log} is stale or needs refresh. Attempting conditional fetch.")
            
            detail_response_content, temp_new_detail_etag, temp_new_detail_last_modified, temp_new_detail_content_length = self.fetch_url(
                detail_url,
                etag=cached_detail_etag,
                last_modified=cached_detail_last_modified,
                cached_content_length=cached_detail_content_length
            )

            if detail_response_content is not None:
                detail_data = self.html_parser.parse_program_details(detail_response_content, program_item_data)
                detail_data['detail_cached_at'] = datetime.now(timezone.utc).isoformat()
                detail_data['detail_etag'] = temp_new_detail_etag
                detail_data['detail_last_modified'] = temp_new_detail_last_modified
                detail_data['detail_content_length'] = temp_new_detail_content_length
                logger.debug(f"Detail page for {item_id_for_log} fetched and parsed new content. Cache timestamp updated.")
                new_detail_etag = temp_new_detail_etag
                new_detail_last_modified = temp_new_detail_last_modified
                new_detail_content_length = temp_new_detail_content_length
            else:
                logger.debug(f"Detail page for {item_id_for_log} validated as not modified/within tolerance. Preserving original cache timestamp.")

        if original_image_url and self.check_image:
            checked_url, status = self._check_image_url_head_threaded(original_image_url, item_id_for_log)
            detail_data['image_check_final_url'] = checked_url
            detail_data['image_check_status'] = status
        elif original_image_url and not self.check_image:
            detail_data['image_check_status'] = 'not_checked'

        return detail_data, new_detail_etag, new_detail_last_modified, new_detail_content_length


    def _process_channel_day_schedule(self, channel_info: Dict[str, str], current_date: datetime.date) -> List[Dict[str, Any]]:
        """
        Fetches and parses a day's schedule for a channel, with retry logic for
        application-level exceptions during schedule parsing.
        It prioritizes fresh data from the local JSON cache. If stale or missing,
        it performs a conditional HTTP GET to check for updates. If cached data's
        timezone differs from requested, it reprocesses timestamps.

        :param channel_info: Dictionary containing channel information ('name', 'url', 'source_id').
        :type channel_info: Dict[str, str]
        :param current_date: The date for which the schedule is being parsed.
        :type current_date: datetime.date
        :returns: A list of dictionaries, each representing a program item.
        :rtype: List[Dict[str, Any]]
        """
        if self.stop_event.is_set():
            logger.debug(f"Stop event set. Aborting _process_channel_day_schedule for {channel_info['name']} on {current_date.strftime('%Y-%m-%d')}.")
            return []

        channel_name: str = channel_info['name']
        channel_url: str = channel_info['url']
        channel_source_id: str = channel_info['source_id']

        items_for_day: List[Dict[str, Any]] = []
        processed_items: List[Dict[str, Any]] = []
        
        new_etag: Optional[str] = None
        new_last_modified: Optional[str] = None
        fetched_content_length: Optional[int] = None
        
        original_cached_at_str: Optional[str] = None

        cached_data_from_file: Optional[Dict[str, Any]] = None
        cached_data: Optional[List[Dict[str, Any]]] = None
        cached_etag: Optional[str] = None
        cached_last_modified: Optional[str] = None
        cached_content_length: Optional[int] = None
        cached_xmltv_timezone: Optional[str] = None
        is_cache_fresh: bool = False
        should_save_cache: bool = False


        if self.enable_cache and not self.cache_clear_requested:
            filepath = self._get_daily_cache_filepath(channel_source_id, current_date)
            cached_data_from_file = self._load_json_with_lock(filepath)

            if cached_data_from_file:
                cached_data = cached_data_from_file.get('data')
                cached_etag = cached_data_from_file.get('etag')
                cached_last_modified = cached_data_from_file.get('last_modified')
                original_cached_at_str = cached_data_from_file.get('cached_at')
                cached_content_length = cached_data_from_file.get('content_length')
                if cached_content_length is None:
                    cached_content_length = 0
                cached_xmltv_timezone = cached_data_from_file.get('xmltv_timezone')

                new_etag = cached_etag
                new_last_modified = cached_last_modified
                fetched_content_length = cached_content_length

                is_cache_fresh = self._is_cache_fresh(original_cached_at_str)

                if is_cache_fresh and cached_data is not None:
                    if cached_xmltv_timezone == self.xmltv_timezone:
                        items_for_day = cached_data
                        logger.debug(f"Using fresh data from cache for {channel_name} on {current_date.strftime('%Y-%m-%d')}.")
                    else:
                        logger.info(f"Cached data for {channel_name} on {current_date.strftime('%Y-%m-%d')} is fresh but timezone differs (cached: {cached_xmltv_timezone}, requested: {self.xmltv_timezone}). Reprocessing timestamps.")
                        items_for_day = cached_data
                        should_save_cache = True
                elif cached_data is not None:
                    logger.debug("Channel list cache is stale. Attempting conditional fetch.")
                else:
                    logger.debug("Channel list cache found but data is malformed. Performing full fetch.")
            else:
                logger.debug("Channel list cache not found or unreadable. Performing full fetch.")
        else:
            if self.cache_clear_requested:
                logger.debug(f"Cache clear requested. Bypassing cache load for {channel_name} on {current_date.strftime('%Y-%m-%d')}.")
            elif not self.enable_cache:
                logger.debug(f"Cache disabled. Bypassing cache load for {channel_name} on {current_date.strftime('%Y-%m-%d')}.")

        if not items_for_day or not is_cache_fresh or (is_cache_fresh and cached_xmltv_timezone != self.xmltv_timezone):
            schedule_url: str = self._build_schedule_url(channel_url, current_date)
            for attempt in range(self.max_schedule_retries + 1):
                if self.stop_event.is_set():
                    logger.debug(f"Stop event set. Aborting schedule fetch attempts for {channel_name} on {current_date.strftime('%Y-%m-%d')}.")
                    return []
                try:
                    attempt_str: str = f" (Attempt {attempt + 1}/{self.max_schedule_retries + 1})" if attempt > 0 else ""
                    logger.debug(f"Attempting to fetch schedule for {channel_name} on {current_date.strftime('%Y-%m-%d')} from {schedule_url}{attempt_str}")

                    response_content, temp_new_etag, temp_new_last_modified, temp_fetched_content_length_from_fetch = self.fetch_url(
                        schedule_url,
                        etag=cached_etag,
                        last_modified=cached_last_modified,
                        cached_content_length=cached_content_length
                    )

                    if response_content is not None:
                        logger.debug(f"HTML content for {channel_name} on {current_date.strftime('%Y-%m-%d')} was modified or fetched for the first time. Parsing new data.")
                        items_for_day = self.html_parser.parse_daily_schedule_items(
                            response_content,
                            channel_info,
                            current_date
                        )
                        for item_data in items_for_day:
                            item_data.update({
                                'detail_etag': None,
                                'detail_last_modified': None,
                                'detail_content_length': None,
                                'detail_cached_at': None,
                                'image_check_final_url': None,
                                'image_check_status': None
                            })
                        should_save_cache = True
                        new_etag = temp_new_etag
                        new_last_modified = temp_new_last_modified
                        fetched_content_length = temp_fetched_content_length_from_fetch
                        break
                    elif cached_data is not None:
                        logger.debug(f"HTML content for {channel_name} on {current_date.strftime('%Y-%m-%d')} not modified (304) or semantically identical. Using stale cache.")
                        items_for_day = cached_data.copy()
                        for item_data in items_for_day:
                            detail_cached_at_str = item_data.get('detail_cached_at')
                            if not self._is_cache_fresh(detail_cached_at_str):
                                logger.debug(f"Detail cache for '{item_data.get('title', 'N/A')}' is stale. Forcing re-fetch.")
                                item_data['detail_cached_at'] = None
                                should_save_cache = True
                        break
                    else:
                        logger.warning(f"Could not retrieve or parse program for {channel_name} on {current_date.strftime('%Y-%m-%d')}. No data available. Retrying...")
                        if attempt < self.max_schedule_retries:
                            time.sleep(1 * (attempt + 1))
                        else:
                            logger.error(f"Max retries ({self.max_schedule_retries}) exhausted for schedule generation for {channel_name} on {current_date.strftime('%Y-%M-%d')}. Skipping this day.")
                            return []
                except KeyboardInterrupt:
                    self.stop_event.set()
                    logger.info(f"KeyboardInterrupt caught in worker for {channel_name} on {current_date.strftime('%Y-%m-%d')}. Signaling main thread to stop.")
                    raise
                except Exception as exc:
                    logger.error(f"Schedule generation for {channel_name} on {current_date.strftime('%Y-%m-%d')} generated an exception on attempt {attempt + 1}: {exc}", exc_info=True)
                    if attempt < self.max_schedule_retries:
                        time.sleep(1 * (attempt + 1))
                    else:
                        logger.error(f"Max retries ({self.max_schedule_retries}) exhausted for schedule generation for {channel_name} on {current_date.strftime('%Y-%m-%d')}. Skipping this day.")
                        return []

        if self.stop_event.is_set():
            return []

        detail_futures = {}
        image_check_futures = {}

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            for index, item_data in enumerate(items_for_day):
                if item_data.get('image_url'):
                    item_data['image_url'] = self._strip_im_parameter_for_cache(item_data['image_url'])

                needs_detail_fetch = False
                if item_data.get('link'):
                    if not self.enable_cache or self.cache_clear_requested:
                        needs_detail_fetch = True
                    elif not self._is_cache_fresh(item_data.get('detail_cached_at')):
                        needs_detail_fetch = True

                needs_image_check_standalone = False
                if self.check_image and item_data.get('image_url') and not needs_detail_fetch:
                    if item_data.get('image_check_status') != 'avail':
                        needs_image_check_standalone = True

                if needs_detail_fetch:
                    item_id_for_log = f"{channel_source_id}-{current_date.strftime('%Y%m%d')}-{item_data.get('starttime', 'XX')}-{self.html_parser._normalize_string_for_comparison(item_data.get('title', ''))[:10]}"
                    future = executor.submit(
                        self._fetch_detail_and_image_threaded,
                        item_data['link'], item_data.copy(), item_data.get('image_url'), item_id_for_log
                    )
                    detail_futures[future] = index
                elif needs_image_check_standalone:
                    item_id_for_log = f"{channel_source_id}-{current_date.strftime('%Y%m%d')}-{item_data.get('starttime', 'XX')}-{self.html_parser._normalize_string_for_comparison(item_data.get('title', ''))[:10]}"
                    future = executor.submit(self._check_image_url_head_threaded, item_data['image_url'], item_id_for_log)
                    image_check_futures[future] = index
                else:
                    if not self.check_image and item_data.get('image_url'):
                        item_data['image_check_status'] = 'not_checked'

            all_futures = list(detail_futures.keys()) + list(image_check_futures.keys())

            for future in concurrent.futures.as_completed(all_futures):
                if self.stop_event.is_set():
                    logger.info("Stop event detected. Shutting down executor and cancelling remaining tasks.")
                    executor.shutdown(wait=False, cancel_futures=True)
                    break

                try:
                    if future in detail_futures:
                        index = detail_futures[future]
                        detailed_data, _, _, _ = future.result()
                        items_for_day[index].update(detailed_data)
                        should_save_cache = True 
                    elif future in image_check_futures:
                        index = image_check_futures[future]
                        checked_url, status = future.result()
                        items_for_day[index]['image_check_final_url'] = checked_url
                        items_for_day[index]['image_check_status'] = status

                except concurrent.futures.CancelledError:
                    index_for_log = detail_futures.get(future) or image_check_futures.get(future)
                    item_title = items_for_day[index_for_log].get('title', 'N/A') if index_for_log is not None else 'N/A'
                    logger.info(f"Task for item {item_title} on {current_date.strftime('%Y-%m-%d')} was cancelled.")
                except Exception as exc:
                    index_for_log = detail_futures.get(future) or image_check_futures.get(future)
                    item_title = items_for_day[index_for_log].get('title', 'N/A') if index_for_log is not None else 'N/A'
                    logger.error(f"Error processing item {item_title} on {current_date.strftime('%Y-%m-%d')}: {exc}", exc_info=True)


        for item_data in items_for_day:
            if self.stop_event.is_set():
                logger.debug(f"Stop event set. Aborting final item processing for {channel_name}.")
                return []
            item_data['rating_cleaned'] = _process_rating_string(item_data.get('rating'))
            item_data['starttime_xmltv'] = _convert_unix_to_xmltv_time(item_data.get('starttime'), self.xmltv_timezone)
            item_data['endtime_xmltv'] = _convert_unix_to_xmltv_time(item_data.get('endtime'), self.xmltv_timezone)
            processed_items.append(item_data)

        if self.enable_cache and should_save_cache:
            cache_save_timestamp = datetime.now(timezone.utc).isoformat()
            self._save_daily_json_cache(
                channel_source_id,
                current_date,
                processed_items,
                new_etag,
                new_last_modified,
                fetched_content_length,
                self.xmltv_timezone,
                cached_at_to_use=cache_save_timestamp
            )
        return processed_items

    def _proactive_cache_cleanup(self) -> None:
        """
        Performs a proactive cleanup of old and irrelevant cache directories and files.
        This includes:
        - Deleting channel-specific subdirectories that are empty.
        - Deleting channel-specific subdirectories for channels that are no longer targeted.
        - Deleting daily JSON cache files for dates beyond MAX_DAYS_TO_SCRAPE from the current date.
        - Deleting daily JSON cache files for past days if --keep-past-cache is not set.
        - Deleting any orphaned temporary .tmp.* files within the cache directory structure.
        """
        logger.info("Starting proactive cache cleanup.")
        current_date = datetime.now().date()

        earliest_relevant_date = current_date - timedelta(days=1)
        latest_relevant_date = current_date + timedelta(days=MAX_DAYS_TO_SCRAPE - 1)

        if os.path.exists(self.processed_data_cache_base_dir):
            for channel_dir_name in os.listdir(self.processed_data_cache_base_dir):
                channel_dir_path = os.path.join(self.processed_data_cache_base_dir, channel_dir_name)
                if os.path.isdir(channel_dir_path):
                    if not os.listdir(channel_dir_path):
                        try:
                            os.rmdir(channel_dir_path)
                            logger.debug(f"Removed empty cache directory: {channel_dir_path}")
                        except OSError as e:
                            logger.warning(f"Failed to remove empty cache directory {channel_dir_path}: {e}")
                        continue

                    if self.target_sourceids and channel_dir_name.lower() not in self.target_sourceids:
                        logger.debug(f"Removing cache directory for non-targeted channel: {channel_dir_name} at {channel_dir_path}")
                        try:
                            shutil.rmtree(channel_dir_path)
                        except OSError as e:
                            logger.error(f"Failed to remove cache directory for non-targeted channel {channel_dir_path}: {e}")
                        continue

                    for filename in os.listdir(channel_dir_path):
                        filepath = os.path.join(channel_dir_path, filename)
                        if filename.endswith('.json'):
                            file_date_str = filename.replace('.json', '')
                            try:
                                file_date = datetime.strptime(file_date_str, '%Y%m%d').date()
                                if (not self.keep_past_cache and file_date < earliest_relevant_date) or \
                                   file_date > latest_relevant_date:
                                    logger.debug(f"Deleting outdated cache file: {filepath}")
                                    try:
                                        os.remove(filepath)
                                    except OSError as e:
                                        logger.warning(f"Failed to delete outdated cache file {filepath}: {e}")
                            except ValueError:
                                logger.debug(f"Skipping non-date named file in cache: {filename}")
                        elif filename.startswith('.') and '.tmp.' in filename:
                            logger.debug(f"Deleting orphaned temporary file: {filepath}")
                            try:
                                os.remove(filepath)
                            except OSError as e:
                                logger.warning(f"Failed to delete orphaned temporary file {filepath}: {e}")
                elif channel_dir_path.startswith('.') and '.tmp.' in channel_dir_path:
                    logger.debug(f"Deleting orphaned temporary file (top-level): {channel_dir_path}")
                    try:
                        os.remove(channel_dir_path)
                    except OSError as e:
                        logger.warning(f"Failed to delete orphaned temporary file {channel_dir_path}: {e}")

        logger.info("Proactive cache cleanup completed.")


    def run_scraper(self) -> List[Dict[str, Any]]:
        """
        Orchestrates the main scraping process.
        Fetches the list of channels, then concurrently scrapes the schedule
        for each channel for the specified number of days.

        :returns: A list of all extracted program items.
        :rtype: List[Dict[str, Any]]
        """
        logger.info("Starting scrape process...")

        if not self.is_specific_date_set:
            logger.info(f"Scraping {self.days_to_scrape} day(s).")

        if self.enable_cache and not self.cache_clear_requested:
            self._proactive_cache_cleanup()

        available_channels_full_list: List[Dict[str, str]] = self._get_channel_list()
        if self.stop_event.is_set():
            return []
        available_channel_ids: set[str] = {channel['source_id'] for channel in available_channels_full_list}

        if self.target_sourceids:
            missing_ids: set[str] = self.target_sourceids - available_channel_ids
            for missing_id in missing_ids:
                logger.warning(f"Requested channel ID '{missing_id}' not found on TVSpielfilm.de. This channel will be skipped.")

            channel_list_to_scrape: List[Dict[str, str]] = [
                channel for channel in available_channels_full_list
                if channel['source_id'] in self.target_sourceids
            ]
            if not channel_list_to_scrape:
                logger.warning("No requested channels found on the website. Exiting.")
                return []
        else:
            channel_list_to_scrape = available_channels_full_list

        if not channel_list_to_scrape:
            logger.warning("No channels found or specified. Exiting.")
            return []

        total_channels: int = len(channel_list_to_scrape)

        all_extracted_items: List[Dict[str, Any]] = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            try:
                for i, channel in enumerate(channel_list_to_scrape):
                    if self.stop_event.is_set():
                        logger.info("Stop event detected. Aborting further channel processing.")
                        break

                    channel_num: int = i + 1
                    logger.info(f"Scraping channel: {channel['name']} (ID: {channel['source_id']}) [{channel_num}/{total_channels}]")

                    channel_tasks: List[Tuple[Dict[str, str], datetime.date]] = []
                    for j in range(self.days_to_scrape):
                        current_date: datetime.date = self.start_date + timedelta(days=j)
                        channel_tasks.append((channel, current_date))

                    future_to_day_task: Dict[concurrent.futures.Future, Tuple[Dict[str, str], datetime.date]] = {
                        executor.submit(self._process_channel_day_schedule, ch, date): (ch, date)
                        for ch, date in channel_tasks
                    }

                    for future in concurrent.futures.as_completed(future_to_day_task):
                        if self.stop_event.is_set():
                            logger.info(f"Stop event detected for channel {channel['name']}. Cancelling remaining tasks for this channel.")
                            break

                        ch_info, current_date_for_task = future_to_day_task[future]
                        try:
                            items_for_day: List[Dict[str, Any]] = future.result()
                            all_extracted_items.extend(items_for_day)
                            logger.info(f"Finished processing items for {ch_info['name']} on {current_date_for_task.strftime('%Y-%m-%d')}. Extracted {len(items_for_day)} items.")
                        except concurrent.futures.CancelledError:
                            logger.info(f"Task for {ch_info['name']} on {current_date_for_task.strftime('%Y-%m-%d')} was cancelled.")
                        except Exception as exc:
                            logger.error(f"Error processing schedule for channel {ch_info['name']} on {current_date_for_task.strftime('%Y-%m-%d')}: {exc}", exc_info=True)
            except KeyboardInterrupt:
                logger.info("KeyboardInterrupt detected. Shutting down scraper gracefully.")
                self.stop_event.set()
                raise

        logger.info(f"Scraping completed. Total items extracted: {len(all_extracted_items)}")

        return all_extracted_items

    def shutdown(self):
        """
        Performs cleanup operations before exiting the scraper.
        """
        logger.debug("Closing requests session.")
        self.session.close()


def _build_image_url_with_params(original_url: str, target_width: int) -> str:
    """
    Constructs an image URL with specified resize parameters for XMLTV output.
    Any existing crop or resize instructions in the original URL will be removed,
    and only the 'Resize,width={target_width}' parameter will be applied.

    :param original_url: The original image URL.
    :type original_url: str
    :param target_width: The desired width for resizing.
    :type target_width: int
    :returns: The modified image URL.
    :rtype: str
    """
    parsed_url = urlparse(original_url)
    raw_query_string = parsed_url.query

    other_query_parts = []

    if raw_query_string:
        for part in raw_query_string.split('&'):
            if not part.startswith('im='):
                other_query_parts.append(part)

    final_im_parts = []

    final_im_parts.append(f'Resize,width={target_width}')

    im_value_for_url = ';'.join(final_im_parts)

    new_query_parts_for_url = []
    if im_value_for_url:
        new_query_parts_for_url.append(f"im={im_value_for_url}")

    new_query_parts_for_url.extend(other_query_parts)

    new_query_string = '&'.join(new_query_parts_for_url)

    return parsed_url._replace(query=new_query_string).geturl()


def generate_xmltv(data_list: List[Dict[str, Any]], output_file: str, xmltv_timezone: str = 'Europe/Berlin') -> None:
    """
    Generates an XMLTV-compliant XML file from the scraped data using lxml.etree.
    Applies XML character sanitation to all string content.

    :param data_list: A list of dictionaries, each representing a program item.
                      Expected to have 'starttime_xmltv', 'endtime_xmltv', and 'rating_cleaned' fields.
    :type data_list: List[Dict[str, Any]]
    :param output_file: The path to the output XMLTV file.
    :type output_file: str
    :param xmltv_timezone: The timezone string to use for XMLTV header and program times.
    :type xmltv_timezone: str
    :returns: None
    :rtype: None
    :raises Exception: If an error occurs during XMLTV file generation.
    """
    logger.debug(f"Generating XMLTV file: {output_file} using lxml.etree.")

    channels: Dict[str, Dict[str, str]] = {}
    for item in data_list:
        sourceid: Optional[str] = item.get('sourceid')
        channelname: Optional[str] = item.get('channelname')
        if sourceid and sourceid not in channels and channelname:
            channels[sourceid] = {'name': channelname}

    if not data_list:
        logger.warning("No program data to generate XMLTV. Output file will not be created.")
        return

    try:
        target_tz = ZoneInfo(xmltv_timezone)
    except Exception as e:
        logger.error(f"Unknown timezone '{xmltv_timezone}'. Falling back to UTC. Error: {e}")
        target_tz = timezone.utc

    now_in_target_tz = datetime.now(timezone.utc).astimezone(target_tz)
    tv_date_str = now_in_target_tz.strftime('%Y%m%d%H%M%S %z').replace(' ', '')

    tv_element = lxml.etree.Element(
        'tv',
        date=tv_date_str,
        **{
            'source-info-name': _sanitize_xml_string('TVSpielfilm.de') or '',
            'source-info-url': _sanitize_xml_string('https://m.tvspielfilm.de/') or '',
            'generator-info-name': _sanitize_xml_string('TVSpielfilm-Scraper/1.0') or ''
        }
    )

    for sourceid, channel_info in sorted(channels.items()):
        robust_channel_id: str = f"{sourceid.lower()}.tvs"
        channel_elem = lxml.etree.SubElement(tv_element, 'channel', id=robust_channel_id)
        display_name_elem = lxml.etree.SubElement(channel_elem, 'display-name')
        display_name_elem.text = channel_info['name']

    image_sizes_to_generate = {
        150: '1',
        300: '2',
        600: '3'
    }

    for item in data_list:
        start_time_xmltv: Optional[str] = item.get('starttime_xmltv')
        stop_time_xmltv: Optional[str] = item.get('endtime_xmltv')

        if not start_time_xmltv or not item.get('sourceid') or not item.get('title'):
            logger.warning(f"Skipping program due to missing essential data: {item.get('title', 'N/A')} on {item.get('channelname', 'N/A')}")
            continue

        robust_channel_id = f"{item['sourceid'].lower()}.tvs"
        programme_attrib: Dict[str, str] = {
            'start': start_time_xmltv,
            'channel': robust_channel_id
        }
        if stop_time_xmltv:
            programme_attrib['stop'] = stop_time_xmltv

        programme_elem = lxml.etree.SubElement(tv_element, 'programme', **programme_attrib)

        title_elem = lxml.etree.SubElement(programme_elem, 'title')
        title_elem.text = item['title']

        if item.get('subtitle') is not None:
            subtitle_elem = lxml.etree.SubElement(programme_elem, 'sub-title')
            subtitle_elem.text = item['subtitle']

        if item.get('shortdescription'):
            short_desc_elem = lxml.etree.SubElement(programme_elem, 'desc')
            short_desc_elem.text = item['shortdescription']

        if item.get('longdescription'):
            long_desc_elem = lxml.etree.SubElement(programme_elem, 'desc')
            long_desc_elem.text = item['longdescription']

        has_credits = any([
            item.get('director'), item.get('cast'), item.get('screenplay'),
            item.get('presenter'), item.get('commentator'), item.get('guest')
        ])

        if has_credits:
            credits_elem = lxml.etree.SubElement(programme_elem, 'credits')

            if item.get('director'):
                director_elem = lxml.etree.SubElement(credits_elem, 'director')
                director_elem.text = item['director']

            if item.get('cast'):
                cast_entries: List[str] = [c.strip() for c in item['cast'].split(',') if c.strip()]
                for entry in cast_entries:
                    match: Optional[re.Match[str]] = re_cast_entry_match.match(entry)
                    if match:
                        actor_name: str = match.group(1).strip()
                        actor_role: str = match.group(2).strip()
                        actor_elem = lxml.etree.SubElement(credits_elem, 'actor', role=actor_role)
                        actor_elem.text = actor_name
                    else:
                        actor_elem = lxml.etree.SubElement(credits_elem, 'actor')
                        actor_elem.text = entry

            if item.get('screenplay'):
                writers: List[str] = [w.strip() for w in item['screenplay'].split(',') if w.strip()]
                for writer in writers:
                    writer_elem = lxml.etree.SubElement(credits_elem, 'writer')
                    writer_elem.text = writer

            if item.get('presenter'):
                for presenter_name in item['presenter']:
                    presenter_elem = lxml.etree.SubElement(credits_elem, 'presenter')
                    presenter_elem.text = presenter_name

            if item.get('commentator'):
                for commentator_name in item['commentator']:
                    commentator_elem = lxml.etree.SubElement(credits_elem, 'commentator')
                    commentator_elem.text = commentator_name

            if item.get('guest'):
                for guest_name in item['guest']:
                    guest_elem = lxml.etree.SubElement(credits_elem, 'guest')
                    guest_elem.text = guest_name

        if item.get('year'):
            date_elem = lxml.etree.SubElement(programme_elem, 'date')
            date_elem.text = str(item['year'])

        if item.get('category'):
            categories: List[str] = [c.strip() for c in item['category'].split(',') if c.strip()]
            for category in categories:
                category_elem = lxml.etree.SubElement(programme_elem, 'category')
                category_elem.text = category
        elif item.get('genre'):
            categories: List[str] = [c.strip() for c in item['genre'].split(',') if c.strip()]
            for category in categories:
                category_elem = lxml.etree.SubElement(programme_elem, 'category')
                category_elem.text = category

        if item.get('keyword'):
            keyword_elem = lxml.etree.SubElement(programme_elem, 'keyword')
            keyword_elem.text = item['keyword']

        if item.get('duration') is not None:
            length_minutes: int = round(item['duration'] / 60)
            length_elem = lxml.etree.SubElement(programme_elem, 'length', units='minutes')
            length_elem.text = str(length_minutes)

        if item.get('link'):
            url_elem = lxml.etree.SubElement(programme_elem, 'url')
            url_elem.text = item['link']

        if item.get('country'):
            country_elem = lxml.etree.SubElement(programme_elem, 'country')
            country_elem.text = item['country']

        if item.get('episode_num') is not None and item.get('season_num') is not None:
            episode_num_elem = lxml.etree.SubElement(programme_elem, 'episode-num', system='onscreen')
            episode_num_elem.text = f"S{item['season_num']:02d}E{item['episode_num']:02d}"

        if item.get('parentalrating') is not None:
            rating_elem = lxml.etree.SubElement(programme_elem, 'rating', system='FSK')
            value_elem = lxml.etree.SubElement(rating_elem, 'value')
            value_elem.text = str(item['parentalrating'])

        if item.get('numrating') is not None:
            star_rating_elem = lxml.etree.SubElement(programme_elem, 'star-rating', system='TVSpielfilm')
            value_elem = lxml.etree.SubElement(star_rating_elem, 'value')
            value_elem.text = f"{item['numrating']} / 3"

        if item.get('imdbrating'):
            imdb_star_rating_elem = lxml.etree.SubElement(programme_elem, 'star-rating', system='IMDB')
            value_elem = lxml.etree.SubElement(imdb_star_rating_elem, 'value')
            value_elem.text = f"{item['imdbrating']} / 10"

        if item.get('txtrating'):
            review_elem = lxml.etree.SubElement(programme_elem, 'review', type='text', source='Rating Conclusion', reviewer='TVSpielfilm')
            review_elem.text = item['txtrating']

        if item.get('rating_cleaned'):
            review_detailed_elem = lxml.etree.SubElement(programme_elem, 'review', type='text', source='Rating by Genre', reviewer='TVSpielfilm')
            review_detailed_elem.text = item['rating_cleaned']

        if item.get('tipp'):
            review_tipp_elem = lxml.etree.SubElement(programme_elem, 'review', type='text', source='Tipp', reviewer='TVSpielfilm')
            review_tipp_elem.text = item['tipp']

        if item.get('image_url'):
            for width, size_attr_value in image_sizes_to_generate.items():
                processed_image_url = _build_image_url_with_params(
                    item['image_url'],
                    width
                )
                image_elem = lxml.etree.SubElement(programme_elem, 'image', size=size_attr_value, orient='L', system='TVSpielfilm')
                image_elem.text = processed_image_url

    tree = lxml.etree.ElementTree(tv_element)

    try:
        tree.write(output_file, pretty_print=True, encoding='utf-8', xml_declaration=True, doctype='<!DOCTYPE tv SYSTEM "xmltv.dtd">')
        logger.info(f"XMLTV file '{output_file}' successfully generated.")
    except Exception as e:
        logger.error(f"Error generating XMLTV file with lxml.etree: {e}")
        raise


def validate_days(value_str: str) -> int:
    """
    Validates the number of days to scrape.

    :param value_str: The string value from the command line.
    :type value_str: str
    :returns: The validated integer value.
    :rtype: int
    :raises argparse.ArgumentTypeError: If the value is not a valid integer or outside the allowed range.
    """
    try:
        days = int(value_str)
    except ValueError:
        raise argparse.ArgumentTypeError(f"Invalid DAYS value: '{value_str}'. Must be an integer.")

    if days < 1:
        logger.info(f"DAYS value {days} is less than 1. Setting to 1 day (today).")
        return 1
    elif days > MAX_DAYS_TO_SCRAPE:
        logger.warning(f"DAYS value {days} exceeds maximum allowed ({MAX_DAYS_TO_SCRAPE}). Setting to {MAX_DAYS_TO_SCRAPE} days.")
        return MAX_DAYS_TO_SCRAPE
    return days

def validate_date(date_str: str) -> datetime.date:
    """
    Validates the date string and checks if it's within the allowed range.

    :param date_str: The date string in YYYYMMDD format.
    :type date_str: str
    :returns: The validated datetime.date object.
    :rtype: datetime.date
    :raises argparse.ArgumentTypeError: If the date format is invalid or the date is out of range.
    """
    try:
        parsed_date = datetime.strptime(date_str, '%Y%m%d').date()
    except ValueError:
        raise argparse.ArgumentTypeError(f"Invalid DATE format: '{date_str}'. Expected YYYYMMDD (e.g., 20250523).")

    today = datetime.now().date()
    earliest_allowed_date = today - timedelta(days=1)
    latest_allowed_date = today + timedelta(days=MAX_DAYS_TO_SCRAPE - 1)

    if parsed_date < earliest_allowed_date:
        raise argparse.ArgumentTypeError(f"The specified date '{date_str}' is too far in the past. Only dates from '{earliest_allowed_date.strftime('%Y%m%d')}' (yesterday) are allowed.")
    if parsed_date > latest_allowed_date:
        raise argparse.ArgumentTypeError(f"The specified date '{date_str}' is too far in the future. Only dates up to '{latest_allowed_date.strftime('%Y%m%d')}' are allowed (maximum {MAX_DAYS_TO_SCRAPE} days from today).")
    
    return parsed_date


if __name__ == '__main__':
    DEFAULT_OUTPUT_FILE: str = 'tvspielfilm'

    parser = argparse.ArgumentParser(
        description="""
A lean web scraper for TVSpielfilm.de to extract TV program data.
This script uses requests and lxml.html for HTML parsing,
supports caching with manual conditional fetching, parallel fetching,
and handles rate limiting with exponential backoff.
""",
        formatter_class=RawTextHelpFormatter,
        epilog="""
Examples:
  Scrape today's program for all channels and save as XMLTV:
    python3 tvs-scraper.py

  Scrape program for ARD and ZDF for the next 3 days as JSON:
    python3 tvs-scraper.py --channel-ids ARD,ZDF --days 3 --output-format json

  List all available channels:
    python3 tvs-scraper.py --list-channels

  Scrape program for a specific date with debug logging:
    python3 tvs-scraper.py --date 20250523 --log-verbose

  Clear the entire cache before scraping:
    python3 tvs-scraper.py --cache-clear

  Scrape program for today and output XMLTV with Europe/Berlin timezone:
    python3 tvs-scraper.py --xmltv-timezone Europe/Berlin
"""
    )

    parser.add_argument(
        '--list-channels',
        action='store_true',
        help='Lists all available channel IDs and their names, then exits.'
    )
    parser.add_argument(
        '--channel-ids',
        type=str,
        help='Comma-separated list of channel IDs (e.g., "ARD,ZDF"). If not specified, all channels will be scraped.'
    )
    parser.add_argument(
        '--channel-ids-file',
        type=str,
        help='Path to a file containing a comma-separated list of channel IDs. If specified, this option takes precedence over --channel-ids.'
    )

    parser.add_argument(
        '--date',
        type=validate_date,
        dest='start_date_obj',
        help='Specific date to scrape in YYYYMMDD format (e.g., 20250523). If specified, only this date will be scraped and --days will be ignored.'
    )
    parser.add_argument(
        '--days',
        type=validate_days,
        default=DEFAULT_DAYS,
        help=f'Number of days to scrape (1-{MAX_DAYS_TO_SCRAPE}). Default: {DEFAULT_DAYS} (means today only). Ignored if --date is specified.'
    )

    parser.add_argument(
        '--output-file',
        type=str,
        default=DEFAULT_OUTPUT_FILE,
        help=f'Path to the output file. If the default filename is used, the file extension will be automatically appended based on --output-format (e.g., .json for "json", .xml for "xmltv"). If a custom filename is provided, it will be used exactly as specified. Default: "{DEFAULT_OUTPUT_FILE}".'
    )
    parser.add_argument(
        '--output-format',
        type=str,
        default=DEFAULT_OUTPUT_FORMAT,
        choices=['xmltv', 'json'],
        help=f'Output format: "xmltv", "json" (JSON array). Default: {DEFAULT_OUTPUT_FORMAT}.'
    )
    parser.add_argument(
        '--xmltv-timezone',
        type=str,
        default='Europe/Berlin',
        help='Specifies the timezone to use for XMLTV output (e.g., "Europe/Berlin", "America/New_York"). This affects the timestamps in the XMLTV file. Default: "Europe/Berlin".'
    )
    parser.add_argument(
        '--img-size',
        type=str,
        choices=['150', '300', '600'],
        default=DEFAULT_IMAGE_SIZE,
        help=f'Image size to extract ("150", "300" or "600"). Default: {DEFAULT_IMAGE_SIZE}.'
    )
    parser.add_argument(
        '--img-check',
        action='store_true',
        dest='check_image',
        help='If set, an additional HEAD request will be performed to check the validity of image URLs. This may increase scraping time.'
    )

    parser.add_argument(
        '--log-level',
        type=str,
        default='WARNING',
        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
        help='Sets the logging level. Options: DEBUG, INFO, WARNING, ERROR, CRITICAL. Default: WARNING.'
    )
    parser.add_argument(
        '--log-verbose',
        action='store_true',
        dest='verbose',
        help='Enables debug logging output (overrides --log-level to DEBUG).'
    )
    parser.add_argument(
        '--log-syslog',
        action='store_true',
        dest='use_syslog',
        help='Sends log output to Syslog (Linux Logger Utility).'
    )
    parser.add_argument(
        '--log-syslog-tag',
        type=str,
        default=DEFAULT_SYSLOG_TAG,
        dest='syslog_tag',
        help=f'Identifier (tag) for Syslog messages. Default: "{DEFAULT_SYSLOG_TAG}".'
    )

    parser.add_argument(
        '--cache-dir',
        type=str,
        help=f'Specifies a custom directory for cache files. Default is a subdirectory "{DEFAULT_CACHE_SUBDIRECTORY}" in the system temporary directory.'
    )
    parser.add_argument(
        '--cache-clear',
        action='store_true',
        dest='clear_cache',
        help='Clears the entire cache directory (including processed data cache) before scraping begins.'
    )
    parser.add_argument(
        '--cache-disable',
        action='store_true',
        default=DEFAULT_CACHE_DISABLE,
        dest='disable_cache',
        help='Disables caching of processed data to disk. Default: False (cache is enabled).'
    )
    parser.add_argument(
        '--cache-ttl',
        type=int,
        default=DEFAULT_CACHE_TTL_SECONDS,
        help=f'Cache Time To Live in seconds. This defines how long a processed file is considered "fresh" and used directly without re-scraping HTML. Default: {DEFAULT_CACHE_TTL_SECONDS // 3600} hours.'
    )
    parser.add_argument(
        '--cache-validation-tolerance',
        type=int,
        default=DEFAULT_CACHE_VALIDATION_TOLERANCE,
        help=f'Tolerance in bytes for content-length comparison when ETag/Last-Modified fails to return 304. Default: {DEFAULT_CACHE_VALIDATION_TOLERANCE} bytes.'
    )
    parser.add_argument(
        '--cache-keep',
        action='store_true',
        dest='keep_past_cache',
        help='If set, cache files for past days will NOT be automatically deleted. By default, past days\' cache files are deleted.'
    )

    parser.add_argument(
        '--max-workers',
        type=int,
        default=DEFAULT_MAX_WORKERS,
        help=f'Maximum number of concurrent workers for data fetching. Default: {DEFAULT_MAX_WORKERS}.'
    )
    parser.add_argument(
        '--max-retries',
        type=int,
        default=DEFAULT_MAX_RETRIES,
        help=f'Maximum number of retries for failed HTTP requests (e.g., 429, 5xx, connection errors). Default: {DEFAULT_MAX_RETRIES}.'
    )
    parser.add_argument(
        '--min-request-delay',
        type=float,
        default=DEFAULT_MIN_REQUEST_DELAY,
        help=f'Minimum delay in seconds between HTTP requests. Applies only to live fetches. Default: {DEFAULT_MIN_REQUEST_DELAY}s.'
    )
    parser.add_argument(
        '--max-concurrent-requests',
        type=int,
        default=DEFAULT_MAX_CONCURRENT_REQUESTS,
        help=f'Maximum number of concurrent HTTP requests allowed. This acts as a global rate limiter. Default: {DEFAULT_MAX_CONCURRENT_REQUESTS}.'
    )
    parser.add_argument(
        '--max-schedule-retries',
        type=int,
        default=DEFAULT_MAX_SCHEDULE_RETRIES,
        help=f'Maximum number of retries for application-level errors during schedule parsing/generation. Default: {DEFAULT_MAX_SCHEDULE_RETRIES}.'
    )
    parser.add_argument(
        '--timeout-http',
        type=int,
        default=DEFAULT_HTTP_TIMEOUT,
        help=f'Timeout in seconds for all HTTP requests (GET, HEAD). Default: {DEFAULT_HTTP_TIMEOUT}s.'
    )

    args = parser.parse_args()

    if args.verbose:
        numeric_level = logging.DEBUG
        logger.info("Verbose mode enabled: Setting log level to DEBUG.")
    else:
        numeric_level = getattr(logging, args.log_level.upper(), None)
        if not isinstance(numeric_level, int):
            raise ValueError(f'Invalid log level: {args.log_level}')

    for handler in logging.root.handlers[:]:
        logging.root.removeHandler(handler)

    syslog_formatter = logging.Formatter(
        fmt='%(asctime)s ' + f'{args.syslog_tag}: %(message)s',
        datefmt="%b %d %H:%M:%S"
    )
    console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

    if args.use_syslog:
        try:
            syslog_handler = logging.handlers.SysLogHandler(address='/dev/log', facility=logging.handlers.SysLogHandler.LOG_USER)
            syslog_handler.setFormatter(syslog_formatter)
            logging.root.addHandler(syslog_handler)
            logger.info(f"Logging to syslog enabled with tag: '{args.syslog_tag}'.")
        except Exception as e:
            logger.error(f"Failed to set up syslog logging to '/dev/log'. This often means the syslog daemon is not running or '/dev/log' is not accessible: {e}. Falling back to console logging.")
            console_handler = logging.StreamHandler(sys.stderr)
            console_handler.setFormatter(console_formatter)
            logging.root.addHandler(console_handler)
    else:
        console_handler = logging.StreamHandler(sys.stderr)
        console_handler.setFormatter(console_formatter)
        logging.root.addHandler(console_handler)

    logging.root.setLevel(numeric_level)
    logger.setLevel(numeric_level)

    base_cache_path: str = args.cache_dir if args.cache_dir else os.path.join(tempfile.gettempdir(), DEFAULT_CACHE_SUBDIRECTORY)

    if args.clear_cache:
        if os.path.exists(base_cache_path):
            logger.info(f"Clearing cache directory: {base_cache_path}")
            shutil.rmtree(base_cache_path)
            logger.info("Cache cleared.")
        else:
            logger.info(f"No cache found at {base_cache_path} to clear.")

    scraper = TvsLeanScraper(
        channel_ids=args.channel_ids,
        days=args.days,
        image_size=args.img_size,
        check_image=args.check_image,
        start_date_obj=args.start_date_obj,
        cache_dir_path=args.cache_dir,
        max_workers=args.max_workers,
        max_retries=args.max_retries,
        channel_ids_file=getattr(args, 'channel_ids_file', None),
        min_request_delay=args.min_request_delay,
        max_concurrent_requests=args.max_concurrent_requests,
        max_schedule_retries=args.max_schedule_retries,
        disable_cache=args.disable_cache,
        cache_ttl=args.cache_ttl,
        keep_past_cache=args.keep_past_cache,
        cache_clear=args.clear_cache,
        xmltv_timezone=args.xmltv_timezone,
        cache_validation_tolerance=args.cache_validation_tolerance,
        http_timeout=args.timeout_http
    )

    if args.list_channels:
        logger.info("Fetching available channels...")
        channel_list: List[Dict[str, str]] = scraper._get_channel_list()
        if channel_list:
            print("\n--- Available Channels (ID: Name) ---")
            for channel in sorted(channel_list, key=lambda x: x['name'].lower()):
                print(f"{channel['source_id'].lower()}: {channel['name']}")
            print("-------------------------------------\n")
        else:
            logger.warning("No channels found.")
        sys.exit(0)

    start_time: float = time.time()
    extracted_data: List[Dict[str, Any]] = []
    try:
        extracted_data = scraper.run_scraper()
    except KeyboardInterrupt:
        print("\nUser interruption (Ctrl-C). Exiting gracefully.")
        sys.exit(1)
    finally:
        scraper.shutdown()

    end_time: float = time.time()

    elapsed_time: float = end_time - start_time

    hours: float
    minutes: float
    seconds: float
    hours, remainder = divmod(elapsed_time, 3600)
    minutes, seconds = divmod(remainder, 60)

    default_output_file_arg: str = parser.get_default('output_file')
    output_filename: str = args.output_file

    if args.output_format == 'json':
        if output_filename == default_output_file_arg and not output_filename.endswith('.json'):
            output_filename = f"{output_filename}.json"

        logger.info(f"Writing items to {output_filename} in JSON array format.")
        try:
            with open(output_filename, 'w', encoding='utf-8') as f:
                json.dump(extracted_data, f, ensure_ascii=False, indent=4)
            logger.info(f"Data successfully saved. Total {len(extracted_data)} items written.")
        except IOError as e:
            logger.error(f"Error writing JSON output file {output_filename}: {e}")
        except Exception as e:
            logger.error(f"Unexpected error saving JSON data to {output_filename}: {e}")

    elif args.output_format == 'xmltv':
        if output_filename == default_output_file_arg and not output_filename.endswith('.xml'):
            output_filename = f"{output_filename}.xml"

        try:
            generate_xmltv(extracted_data, output_filename, scraper.xmltv_timezone)
        except Exception as e:
            logger.error(f"Failed to generate XMLTV file: {e}")

    else:
        logger.warning("No data extracted or invalid output format specified. Output file will not be created.")

    logger.info(f"Scraping and data processing completed in {int(hours)}h {int(minutes)}m {seconds:.2f}s.")
